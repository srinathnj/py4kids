{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af46672-6ee7-48da-9388-a0d263007eb3",
   "metadata": {},
   "source": [
    "[Embeddings and Vector Databases With ChromaDB](https://realpython.com/chromadb-vector-database/) \n",
    "\n",
    "- Representing unstructured objects with vectors\n",
    "- Using word and text embeddings in Python\n",
    "- Harnessing the power of vector databases\n",
    "- Encoding and querying over documents with ChromaDB\n",
    "- Providing context to LLMs like ChatGPT with ChromaDB\n",
    "\n",
    "[Ode to Joy](https://claude.ai/chat/3883912c-bae6-4f82-85de-2f09382f1c90) a fruitful chat session to be followed up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f82536-2b03-4541-89d4-62c1aa8ae774",
   "metadata": {},
   "source": [
    "## Vector Basics\n",
    "\n",
    "\n",
    "A better way to compute the dot product is to use the at-operator (@), which can perform both vector and matrix multiplications, and the syntax is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb0a961-ba7b-4e9b-a73b-2dc2f39052ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([0, 1])\n",
    "v3 = np.array([np.sqrt(2), np.sqrt(2)])\n",
    "\n",
    "# Dimension\n",
    "v1.shape\n",
    "# (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b9a9e5-e3af-48fb-adaa-30196b2fff84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 2.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Magnitude\n",
    "np.sqrt(np.sum(v1**2)) ,  np.linalg.norm(v1) ,  np.linalg.norm(v3)\n",
    "# 1.0,  1.0, 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7858dd54-f1ba-4c0f-9c2a-fccf41af94e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product\n",
    "np.sum(v1 * v2)\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024d8595-8701-4a5d-b66b-ace51fe23c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.4142135623730951)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 @ v2, v2 @ v3\n",
    "# 1.4142135623730951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0d13b-427a-4bdd-97cf-e3e9d07733c9",
   "metadata": {},
   "source": [
    "## Vector Similarity\n",
    "\n",
    "cosine similarity - a normalized form of the dot product. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc6324-4dbb-4b00-ada6-6216c99a816a",
   "metadata": {},
   "source": [
    "## Encode Objects in Embeddings\n",
    "\n",
    "Embeddings are a way to represent data such as words, text, images, and audio in a numerical format that computational algorithms can more easily process.\n",
    "\n",
    "More specifically, embeddings are dense vectors that characterize meaningful information about the objects that they encode. The most common kinds of embeddings are word and text embeddings, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094f925-73ea-4898-9434-a750f72c2a7f",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "A word embedding is a vector that captures the semantic meaning of word. Ideally, words that are semantically similar in natural language should have embeddings that are similar to each other in the encoded vector space. Analogously, words that are unrelated or opposite of one another should be further apart in the vector space. related words are clustered together, while unrelated words are far from each other.\n",
    "\n",
    "```\n",
    "conda create -n rag python=3.11\n",
    "conda activate rag\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea018b1-8578-481b-b7c5-dc8e67579c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (300,),\n",
       " array([-0.72483 ,  0.42538 ,  0.025489], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "dog_embedding = nlp.vocab[\"dog\"].vector\n",
    "type(dog_embedding),  dog_embedding.shape,  dog_embedding[0:3]\n",
    "# (numpy.ndarray,  (300,), array([-0.72483 ,  0.42538 ,  0.025489], dtype=float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6135a0ce-8893-41c8-a1ab-b64539d8eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"Compute the cosine similarity between two vectors\"\"\"\n",
    "    return (u @ v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e4490d-8a14-40c4-9bf9-c736dc2ca296",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_embedding = nlp.vocab[\"dog\"].vector\n",
    "cat_embedding = nlp.vocab[\"cat\"].vector\n",
    "apple_embedding = nlp.vocab[\"apple\"].vector\n",
    "tasty_embedding = nlp.vocab[\"tasty\"].vector\n",
    "delicious_embedding = nlp.vocab[\"delicious\"].vector\n",
    "truck_embedding = nlp.vocab[\"truck\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c79671-1b7f-4c45-8165-4319baebb7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0000001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(dog_embedding, cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c4d800-3351-44a7-936e-cb2bd5c13ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.450864)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(delicious_embedding, tasty_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2906f741-be02-45e0-a4e6-53eb31d1cdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.39558223)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(apple_embedding, delicious_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2b3dbc-9a7b-4b58-890c-ad98d3292d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.2334378)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(dog_embedding, apple_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d05c1c-5f2a-458f-98e0-8d71f5cd164a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.036047027)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(truck_embedding, delicious_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ebd23-08b1-45ac-96ad-18ce12577e7c",
   "metadata": {},
   "source": [
    "### Text Embeddings\n",
    "\n",
    "Text embeddings encode information about sentences and documents, not just individual words, into vectors. This allows you to compare larger bodies of text to each other just like you did with word vectors. Because they encode more information than a single word embedding, text embeddings are a more powerful representation of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55206653-27b7-470f-94b5-8108b6204668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
