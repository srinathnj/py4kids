{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b687e8ce-eb46-42d6-8fc5-79d760c8d4d7",
   "metadata": {},
   "source": [
    "import psutil\n",
    "\n",
    "for proc in psutil.process_iter(['pid', 'name']):\n",
    "    if 'ollama' in proc.info['name'].lower():\n",
    "        print(proc.info)\n",
    "        # proc.kill()\n",
    "        # # AccessDenied: (pid=55989, name='ollama')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6694ff3a-72d9-45c3-b262-c74140c7529a",
   "metadata": {},
   "source": [
    "for proc in psutil.process_iter(['pid', 'name']):\n",
    "    if 'ollama' in proc.info['name'].lower():\n",
    "        proc.kill()\n",
    "        # AccessDenied: (pid=55989, name='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b014863-6ea5-4a7f-bf41-f6f84cc61b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Ollama service...\n",
      "Starting Ollama service...\n",
      "Ollama service successfully restarted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fresh_start_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fb565c-f9ef-4d68-8d19-47d622e87088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clean up GPU memory using nvidia-smi.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear GPU memory using nvidia-smi\n",
    "        subprocess.run(['nvidia-smi', '--gpu-reset'], check=True)\n",
    "        print(\"GPU memory reset completed\")\n",
    "        \n",
    "        # Get current GPU stats\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"\\nGPU Statistics after cleanup:\")\n",
    "        print(result.stdout)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPU memory cleanup: {str(e)}\")\n",
    "\n",
    "def fresh_start_ollama(wait_time: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    Restarts the Ollama service and cleans up GPU memory.\n",
    "    \n",
    "    Args:\n",
    "        wait_time (int): Time to wait in seconds after restarting Ollama\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False if an error occurred\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, clean up GPU memory\n",
    "        print(\"Cleaning up GPU memory...\")\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        # Stop Ollama service\n",
    "        print(\"Stopping Ollama service...\")\n",
    "        subprocess.run(['sudo', 'systemctl', 'stop', 'ollama'], check=True)\n",
    "        \n",
    "        # Additional GPU cleanup after service stop\n",
    "        cleanup_gpu_memory()\n",
    "        \n",
    "        time.sleep(2)  # Wait for complete shutdown\n",
    "        \n",
    "        # Start Ollama service\n",
    "        print(\"Starting Ollama service...\")\n",
    "        subprocess.run(['sudo', 'systemctl', 'start', 'ollama'], check=True)\n",
    "        \n",
    "        # Wait for service to be ready\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Verify service is running\n",
    "        result = subprocess.run(['systemctl', 'is-active', 'ollama'], \n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "        \n",
    "        if result.stdout.strip() == 'active':\n",
    "            print(\"Ollama service successfully restarted\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Warning: Ollama service might not be fully active\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ollama restart: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def nvidia_smi_stats():\n",
    "    \"\"\"\n",
    "    Get NVIDIA GPU statistics using nvidia-smi.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"\\nGPU Statistics:\")\n",
    "        print(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting GPU stats: {str(e)}\")\n",
    "\n",
    "# Example benchmark function with monitoring\n",
    "def benchmark_with_monitoring(model_name: str, warm_up_prompt: str, actual_prompt: str):\n",
    "    \"\"\"\n",
    "    Run a benchmark with GPU monitoring.\n",
    "    \"\"\"\n",
    "    import ollama\n",
    "    \n",
    "    print(\"\\nStarting fresh with GPU cleanup...\")\n",
    "    fresh_start_ollama(wait_time=5)\n",
    "    \n",
    "    print(\"\\nGPU state before warm-up:\")\n",
    "    nvidia_smi_stats()\n",
    "    \n",
    "    print(\"\\nRunning warm-up query...\")\n",
    "    start_time = time.time()\n",
    "    response = ollama.chat(model=model_name, messages=[\n",
    "        {'role': 'user', 'content': warm_up_prompt}\n",
    "    ])\n",
    "    warm_up_time = time.time() - start_time\n",
    "    print(f\"Warm-up time: {warm_up_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nGPU state after warm-up:\")\n",
    "    nvidia_smi_stats()\n",
    "    \n",
    "    # Small delay between queries\n",
    "    time.sleep(2)\n",
    "    \n",
    "    print(\"\\nRunning actual query...\")\n",
    "    start_time = time.time()\n",
    "    response = ollama.chat(model=model_name, messages=[\n",
    "        {'role': 'user', 'content': actual_prompt}\n",
    "    ])\n",
    "    query_time = time.time() - start_time\n",
    "    print(f\"Query time: {query_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nFinal GPU state:\")\n",
    "    nvidia_smi_stats()\n",
    "    \n",
    "    return {\n",
    "        'warm_up_time': warm_up_time,\n",
    "        'query_time': query_time,\n",
    "        'response': response['message']['content']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8966f98-0707-4c8e-941c-94085999b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n",
      "GPU Reset couldn't run because GPU 00000000:01:00.0 is the primary GPU.\n",
      "Error during GPU memory cleanup: Command '['nvidia-smi', '--gpu-reset']' returned non-zero exit status 3.\n",
      "Stopping Ollama service...\n",
      "GPU Reset couldn't run because GPU 00000000:01:00.0 is the primary GPU.\n",
      "Error during GPU memory cleanup: Command '['nvidia-smi', '--gpu-reset']' returned non-zero exit status 3.\n",
      "Starting Ollama service...\n",
      "Ollama service successfully restarted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fresh_start_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c19c97f-9966-4732-9bbd-88da1110678a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia_smi_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnvidia_smi_stats\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nvidia_smi_stats' is not defined"
     ]
    }
   ],
   "source": [
    "nvidia_smi_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c62df-ecc8-423b-a05a-9978c098029f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
