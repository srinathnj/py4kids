{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e198323-5dfb-459f-8229-e7c1c3103d39",
   "metadata": {},
   "source": [
    "\n",
    "- [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n",
    "\n",
    "- [QWEN2.5-CODER-32B-Instruct](https://www.youtube.com/channel/UCfOvNb3xj28SNqPQ_JIbumg/community?lb=UgkxOmW9TwEVkoTFxfz7kJDUhnh4Eetr2ar5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11fde0dc-2262-44f6-9ac6-acea013442f5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7708d2d6-777c-404d-84de-88364779e142",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685e1f4-097c-42c4-9ce1-0cab31a0e6aa",
   "metadata": {},
   "source": [
    "### verify everything is set up correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a593d19e-7438-4951-93f7-057e81525a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Transformers version: 4.47.0.dev0\n",
      "Bitsandbytes version: 0.44.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1080 Ti\n",
      "CUDA memory allocated: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Bitsandbytes version: {bitsandbytes.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afee1f5-7704-4169-a6d1-cc9e7eade3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get API token from environment variable\n",
    "api_key = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"HUGGING_FACE_HUB_TOKEN not found in environment variables\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=api_key)\n",
    "\n",
    "# Check CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375201fc-c7a3-4372-9201-9a81947a83af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Before Loading: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Before Loading: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32468064-974e-4bdb-91b1-7cdac44bf754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9903d4efb640bdb98de39bc563bcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/31.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693133199c464a6fb9cbdc93d548f47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7080a1a426a74ab990708ff22170b12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c2729aa9fd4baead2b5c7d93ef9f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f0eb6c54f042d796f79fa8aeaf199b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b74e46d4b94dd7a50352f5762a46a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Final GPU Memory: 0.00MB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # For 11GB VRAM, let's use a smaller model or enable quantization\n",
    "    model_name = \"Qwen/Qwen1.5-7B-Chat\"  # Using a smaller 7B model instead of 32B\n",
    "    \n",
    "    # Load model with 8-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use float16 for reduced memory\n",
    "        load_in_4bit=True,  # load_in_8bit=True,         # Enable 8-bit quantization\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = \"write a quick sort algorithm.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Generate response\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Adjust generation parameters for memory efficiency\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        num_beams=1,  # Reduce beam search to save memory\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"Response:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    # Clear CUDA cache after generation\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Final GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d8c42-17fa-439f-b0cc-908f72169657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag)",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
