# Local LLM

## [Ollama](https://ollama.ai/)
general purpose LLM library

follow install at https://github.com/jmorganca/ollama

```
curl https://ollama.ai/install.sh | sh
ollama run llama2
```

## Mozilla [llamafile](https://github.com/Mozilla-Ocho/llamafile#binary-instructions)

1) download and run server llama file from your terminal 
2) access the chat UI in your web browser at https://localhost:8080

## [llmware](https://github.com/llmware-ai/llmware/tree/main)

specialized RAG/LLM python pkg

```
pip install llmware
```

# Usa-cases

## RAG

