{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n",
    "\n",
    "* [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "\n",
    "\n",
    "Other examples\n",
    "- [Spark Data Operations](https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark/blob/master/Section%202%20-%20Working%20with%20PySpark/2.5/2.5%20-%20Spark%20Data%20Operations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"chapter-06-types\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "import os\n",
    "SPARK_BOOK_DATA_PATH = os.environ['SPARK_BOOK_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = SPARK_BOOK_DATA_PATH + \"/data/retail-data/by-day/2010-12-01.csv\"\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |22752    |SET 7 BABUSHKA NESTING BOXES       |2       |2010-12-01 08:26:00|7.65     |17850.0   |United Kingdom|\n",
      "|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |2010-12-01 08:26:00|4.25     |17850.0   |United Kingdom|\n",
      "|536366   |22633    |HAND WARMER UNION JACK             |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT          |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT      |32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3108|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import lit\n",
    "df.select(F.lit(5), F.lit(\"five\"), F.lit(5.0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(F.round(F.lit(\"2.5\")), F.bround(F.lit(\"2.5\")))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "df.where(F.col(\"InvoiceNo\") != 536365)\\\n",
    "  .select(\"InvoiceNo\", \"Description\")\\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import instr\n",
    "priceFilter = F.col(\"UnitPrice\") > 600\n",
    "descripFilter = F.instr(df.Description, \"POSTAGE\") > 0\n",
    "df.where(df.StockCode.isin(\"DOT\") & (priceFilter | descripFilter)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|       true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import instr\n",
    "DOTCodeFilter = F.col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = F.col(\"UnitPrice\") > 600\n",
    "descripFilter = F.instr(F.col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df2 = df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"*\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import expr\n",
    "df3 = df2.withColumn(\"below600\", F.expr(\"UnitPrice < 600\"))\\\n",
    "  .select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|below600|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|    true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|       true|   false|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|below600|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|       true|    true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.where(F.col(\"isExpensive\") & F.col(\"below600\")).select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+------------------+\n",
      "|CustomerId|Quantity|UnitPrice|      fakeQuantity|\n",
      "+----------+--------+---------+------------------+\n",
      "|   17850.0|       6|     2.55|239.08999999999997|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "+----------+--------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = F.pow(F.col(\"Quantity\") * F.col(\"UnitPrice\"), 2) + 5\n",
    "df.select(\"CustomerId\", \"Quantity\", \"UnitPrice\", fabricatedQuantity.alias(\"fakeQuantity\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+------------------+\n",
      "|CustomerId|Quantity|UnitPrice|      fakeQuantity|\n",
      "+----------+--------+---------+------------------+\n",
      "|   17850.0|       6|     2.55|239.08999999999997|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "+----------+--------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    \"(POWER((Quantity * UnitPrice), 2.0) + 5) as fakeQuantity\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+------------------+\n",
      "|CustomerId|Quantity|UnitPrice|      fakeQuantity|\n",
      "+----------+--------+---------+------------------+\n",
      "|   17850.0|       6|     2.55|239.08999999999997|\n",
      "|   17850.0|       6|     3.39|          418.7156|\n",
      "+----------+--------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    CustomerId,\n",
    "    Quantity, \n",
    "    UnitPrice,\n",
    "    (POWER((Quantity * UnitPrice), 2.0) + 5) as fakeQuantity\n",
    "from \n",
    "    dfTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stat.corr()  and crosstab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04112314436835551"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.corr(\"Quantity\", \"UnitPrice\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import count, mean, stddev_pop, min, max\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|StockCode_freqItems                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Quantity_freqItems                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[90214E, 20728, 20755, 21703, 22113, 22524, 22041, 72803A, 72798C, 90181B, 21756, 22694, 90206C, 20970, 21624, 90209C, 84744, 82494L, 22952, 20682, 22583, 21705, 20679, 22220, 90177E, 90214A, 22448, 90214S, 22121, 22802, 84970L, 72818, 90192, 90200C, 22910, 21380, 90211A, 21137, 35271S, 84926A, 20765, 22384, 21524, 22165, 22366, 21221, 21704, 22519, 85035C, 21967, 22114, 22909, 22900, 22447, 21577, 21877, 20726, 85034A, DOT, 84658, 21472, 22804, 22222, 72802C, 21739, 22467, 90214H, 22785, 22446, 22197, 20665, 21733, 22731, 21709, 22086, 40001, 85123A]|[200, 128, 23, 32, 50, 600, 8, 17, 80, -1, -10, 11, 56, 47, 20, -7, 2, 5, 480, -4, 14, 432, 100, 64, 40, 13, 4, -5, 22, 16, -2, 7, 70, 384, 25, 34, 10, 1, 288, 216, 28, 252, 19, 120, 192, 60, 96, 72, 144, 36, 27, 9, 18, 48, 21, 12, 3, -6, -24, 30, 15, 33, 6, 24, -12, -3]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### monotonically_increasing_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "|                            5|\n",
      "|                            6|\n",
      "|                            7|\n",
      "|                            8|\n",
      "|                            9|\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(F.monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country| Id|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|  0|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  1|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|  2|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  3|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  4|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# add Id column\n",
    "df = df.withColumn(\"Id\", F.monotonically_increasing_id())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initcap, lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import initcap\n",
    "df.select(F.initcap(F.col(\"Description\"))).show(5, False)  # False - display column with unlimited width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |initcap(Description)              |lower(Description)                |upper(Description)                |\n",
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|White Hanging Heart T-light Holder|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |White Metal Lantern               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import lower, upper\n",
    "(\n",
    "df.select(F.col(\"Description\"),\n",
    "    F.initcap(F.col(\"Description\")),\n",
    "    F.lower(F.col(\"Description\")),\n",
    "    F.upper(F.col(\"Description\")))\n",
    "    .show(2, False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trim(), pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+---+----------+\n",
      "|    ltrim|    rtrim| trim| lp|        rp|\n",
      "+---------+---------+-----+---+----------+\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "+---------+---------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    F.ltrim(F.lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    F.rtrim(F.lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    F.trim(F.lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    F.lpad(F.lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    F.rpad(F.lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LEET, 1337)|Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import translate\n",
    "df.select(F.translate(F.col(\"Description\"), \"LEET\", \"1337\"), F.col(\"Description\"))\\\n",
    "  .show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "    F.regexp_replace(F.col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    F.col(\"Description\"))\\\n",
    "    .show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|color_clean|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import regexp_extract\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "     F.regexp_extract(F.col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "     F.col(\"Description\"))\\\n",
    "    .show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------+\n",
      "|Description                       |hasSimpleColor|\n",
      "+----------------------------------+--------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true          |\n",
      "|WHITE METAL LANTERN               |true          |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |true          |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true          |\n",
      "|WHITE METAL LANTERN               |true          |\n",
      "+----------------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import instr\n",
    "containsBlack = F.instr(F.col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = F.instr(F.col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    "  .where(\"hasSimpleColor\")\\\n",
    "  .select(\"Description\", \"hasSimpleColor\")\\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### locate() - construct columns dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import expr, locate\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "  return F.locate(color_string.upper(), column)\\\n",
    "          .cast(\"boolean\")\\\n",
    "          .alias(\"is_\" + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(F.expr(\"*\")) # has to a be Column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------+------+\n",
      "|Description                       |is_white|is_red|\n",
      "+----------------------------------+--------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true    |false |\n",
      "|WHITE METAL LANTERN               |true    |false |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |true    |true  |\n",
      "+----------------------------------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(*selectedColumns).where(F.expr(\"is_white OR is_red\"))\\\n",
    "  .select(\"Description\",\"is_white\",\"is_red\")\\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime\n",
    "\n",
    "- current_date()  \n",
    "- current_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2021-04-01|2021-04-01 06:39:50.797|\n",
      "|1  |2021-04-01|2021-04-01 06:39:50.797|\n",
      "|2  |2021-04-01|2021-04-01 06:39:50.797|\n",
      "|3  |2021-04-01|2021-04-01 06:39:50.797|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    "  .withColumn(\"today\", F.current_date())\\\n",
    "  .withColumn(\"now\", F.current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "\n",
    "dateDF.show(4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to_date(), to_timestamp(), date_add(), date_sub(), datediff(), months_between()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------+------------------+\n",
      "| id|date_sub(today, 5)|     today|date_add(today, 5)|\n",
      "+---+------------------+----------+------------------+\n",
      "|  0|        2021-03-27|2021-04-01|        2021-04-06|\n",
      "|  1|        2021-03-27|2021-04-01|        2021-04-06|\n",
      "+---+------------------+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "dateDF.select(\"id\", F.date_sub(F.col(\"today\"), 5), \"today\", F.date_add(F.col(\"today\"), 5))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", F.date_sub(F.col(\"today\"), 7))\\\n",
    "    .select(F.datediff(F.col(\"week_ago\"), F.col(\"today\")))\\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|     start|       end|  month_diff|\n",
      "+----------+----------+------------+\n",
      "|2016-01-01|2017-05-22|-16.67741935|\n",
      "+----------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(\n",
    "    F.to_date(F.lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    F.to_date(F.lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "    .select(\"start\",\"end\",F.months_between(F.col(\"start\"), F.col(\"end\")).alias(\"month_diff\"))\\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|     start|       end|  month_diff|\n",
      "+----------+----------+------------+\n",
      "|2016-01-01|2017-05-22|-16.67741935|\n",
      "+----------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dateDF\n",
    "     .withColumn(\"start\", F.to_date(F.lit(\"2016-01-01\")))\n",
    "     .withColumn(\"end\", F.to_date(F.lit(\"2017-05-22\")))\n",
    "     .withColumn(\"month_diff\", F.months_between(F.col(\"start\"), F.col(\"end\")))\n",
    "     .select(\"start\", \"end\", \"month_diff\")\n",
    "     .show(1)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     date1|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    F.to_date(F.lit(\"2017-12-11\"), dateFormat).alias(\"date1\"),\n",
    "    F.to_date(F.lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "cleanDateDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     date1|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dateTable2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|to_timestamp(`date1`, 'yyyy-dd-MM')|\n",
      "+-----------------------------------+\n",
      "|                2017-11-12 00:00:00|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(F.to_timestamp(F.col(\"date1\"), dateFormat))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### na.drop(),  na.fill(), na.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"Description\") == '').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, Id: bigint]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex type\n",
    "\n",
    "\n",
    "#### struct()\n",
    "\n",
    "combine multiple columns into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import struct\n",
    "complexDF = df.select(F.struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |\n",
      "|[WHITE METAL LANTERN, 536365]                |\n",
      "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from complexDF\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split\n",
    "\n",
    "convert one column into array type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+\n",
      "|Description                       |desc_words                              |\n",
      "+----------------------------------+----------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import split\n",
    "df.select(\"Description\", F.split(F.col(\"Description\"), \" \").alias(\"desc_words\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------------+------------+\n",
      "|Description                        |array_col                                 |array_col[0]|array_col[1]|\n",
      "+-----------------------------------+------------------------------------------+------------+------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |WHITE       |HANGING     |\n",
      "|WHITE METAL LANTERN                |[WHITE, METAL, LANTERN]                   |WHITE       |METAL       |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |CREAM       |CUPID       |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|KNITTED     |UNION       |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |RED         |WOOLLY      |\n",
      "+-----------------------------------+------------------------------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.withColumn(\"array_col\", F.split(F.col(\"Description\"), \" \"))\\\n",
    "    .selectExpr(\"Description\", \"array_col\", \"array_col[0]\",\"array_col[1]\")\\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------+\n",
      "|Description                       |arr_size|\n",
      "+----------------------------------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|5       |\n",
      "|WHITE METAL LANTERN               |3       |\n",
      "+----------------------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import size\n",
    "df.select(\"Description\",\n",
    "    F.size(F.split(F.col(\"Description\"), \" \")).alias(\"arr_size\"))\\\n",
    "    .show(2, False) # shows 5 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+\n",
      "|Description                       |has_white|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true     |\n",
      "|WHITE METAL LANTERN               |true     |\n",
      "+----------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import array_contains\n",
    "df.select(\"Description\",\n",
    "        F.array_contains(F.split(F.col(\"Description\"), \" \"), \"WHITE\").alias(\"has_white\")\n",
    "    ).show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode\n",
    "\n",
    "denorm array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "|WHITE METAL LANTERN               |536365   |WHITE   |\n",
      "|WHITE METAL LANTERN               |536365   |METAL   |\n",
      "|WHITE METAL LANTERN               |536365   |LANTERN |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365   |CREAM   |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365   |CUPID   |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn(\"splitted\", F.split(F.col(\"Description\"), \" \"))\\\n",
    "  .withColumn(\"exploded\", F.explode(F.col(\"splitted\")))\\\n",
    "  .select(\"Description\", \"InvoiceNo\", \"exploded\")\\\n",
    "  .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map\n",
    "\n",
    "create a hash map between 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "|Description                        |InvoiceNo|complex_map                                    |\n",
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE HANGING HEART T-LIGHT HOLDER -> 536365] |\n",
      "|WHITE METAL LANTERN                |536365   |[WHITE METAL LANTERN -> 536365]                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM CUPID HEARTS COAT HANGER -> 536365]     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED UNION FLAG HOT WATER BOTTLE -> 536365]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |[RED WOOLLY HOTTIE WHITE HEART. -> 536365]     |\n",
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import create_map\n",
    "df.select(\"Description\", \"InvoiceNo\", F.create_map(F.col(\"Description\"), F.col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.select(F.create_map(F.col(\"Description\"), F.col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .selectExpr(\"complex_map['WHITE METAL LANTERN']\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+----------------------------------+------+\n",
      "|Description                       |InvoiceNo|key                               |value |\n",
      "+----------------------------------+---------+----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE HANGING HEART T-LIGHT HOLDER|536365|\n",
      "|WHITE METAL LANTERN               |536365   |WHITE METAL LANTERN               |536365|\n",
      "+----------------------------------+---------+----------------------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.withColumn(\"complex_map\", F.create_map(F.col(\"Description\"), F.col(\"InvoiceNo\")))\\\n",
    "    .selectExpr(\"Description\", \"InvoiceNo\", \"explode(complex_map)\")\\\n",
    "    .show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|column|c0                     |\n",
      "+------+-----------------------+\n",
      "|2     |{\"myJSONValue\":[1,2,3]}|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    F.get_json_object(F.col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    F.json_tuple(F.col(\"jsonString\"), \"myJSONKey\")\n",
    "    ).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pack columns into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|structstojson(myStruct)                                                  |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "  .select(F.to_json(F.col(\"myStruct\")))\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|old_json                                    |newJSON                                                                  |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|[536365, WHITE HANGING HEART T-LIGHT HOLDER]|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|[536365, WHITE METAL LANTERN]               |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import from_json\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType((\n",
    "  StructField(\"InvoiceNo\",StringType(),True),\n",
    "  StructField(\"Description\",StringType(),True)))\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "  .select(F.to_json(F.col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    "  .select(F.from_json(F.col(\"newJSON\"), parseSchema).alias(\"old_json\"), F.col(\"newJSON\"))\\\n",
    "    .show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power3(double_value):\n",
    "  return float(double_value ** 3)\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import udf\n",
    "power3udf = F.udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|num|num_cubed|\n",
      "+---+---------+\n",
      "|  0|      0.0|\n",
      "|  1|      1.0|\n",
      "|  2|      8.0|\n",
      "|  3|     27.0|\n",
      "|  4|     64.0|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "udfExampleDF\\\n",
    "    .select(\"num\", power3udf(F.col(\"num\")).alias(\"num_cubed\"))\\\n",
    "    .show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|          0.0|\n",
      "|          1.0|\n",
      "|          8.0|\n",
      "|         27.0|\n",
      "|         64.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(5)\n",
    "# registered via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|power3py|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show user functions like 'power*'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample question for certification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create spark dataframe from list\n",
    "\n",
    "https://stackoverflow.com/questions/43444925/how-to-create-dataframe-from-list-in-spark-sql/50969995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             Words|Score|\n",
      "+------------------+-----+\n",
      "|             Hello|    1|\n",
      "|         I am fine|    3|\n",
      "|Become Spark Smart|  100|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_schema = StructType([\n",
    "                StructField(\"Words\", StringType())\n",
    "               ,StructField(\"Score\", IntegerType())\n",
    "              ])\n",
    "\n",
    "test_list = [['Hello', 1], \n",
    "             ['I am fine', 3], \n",
    "             ['Become Spark Smart', 100]\n",
    "            ]\n",
    "\n",
    "test_df = spark.createDataFrame(test_list, schema=test_schema) \n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import (col,count,desc,sum)\n",
    "\n",
    "a = [1002, 3001, 4002, 2003, 2002, 3004, 1003, 4006]\n",
    "# b = spark.createDataFrame(list(map(lambda x: Row(value=x), a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (spark\n",
    "  .createDataFrame(list(map(lambda x: Row(value=x), a)))\n",
    "  .withColumn(\"x\", F.col(\"value\") % 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value|  x|\n",
      "+-----+---+\n",
      "| 1002|  2|\n",
      "| 3001|  1|\n",
      "| 4002|  2|\n",
      "| 2003|  3|\n",
      "| 2002|  2|\n",
      "| 3004|  4|\n",
      "| 1003|  3|\n",
      "| 4006|  6|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|total|\n",
      "+-----+-----+\n",
      "|    3| 7006|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = (\n",
    "    b\n",
    "    .groupBy(col(\"x\"))\n",
    "    .agg(count(\"x\"), sum(\"value\"))\n",
    "    .drop(\"x\")\n",
    "    .toDF(\"count\", \"total\")\n",
    "    .orderBy(col(\"count\").desc(), col(\"total\"))\n",
    "    .limit(1)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|total|\n",
      "+-----+-----+\n",
      "|    3| 7006|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = b\\\n",
    "    .groupBy(col(\"x\"))\\\n",
    "    .agg(count(\"x\"), sum(\"value\"))\\\n",
    "    .drop(\"x\")\\\n",
    "    .toDF(\"count\", \"total\")\\\n",
    "    .orderBy(col(\"count\").desc(), col(\"total\"))\\\n",
    "    .limit(1)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----+\n",
      "|UserKey|ItemKey|ItemName|Score|\n",
      "+-------+-------+--------+-----+\n",
      "|      1|   1000|   Apple| 0.76|\n",
      "|      2|   1000|   Apple| 0.11|\n",
      "|      1|   2000|  Orange| 0.98|\n",
      "|      1|   3000|  Banana| 0.24|\n",
      "|      2|   3000|  Banana| 0.99|\n",
      "+-------+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = StructType([\n",
    "                  StructField(\"UserKey\", IntegerType())\n",
    "                 ,StructField(\"ItemKey\", IntegerType())\n",
    "                 ,StructField(\"ItemName\", StringType())\n",
    "                 ,StructField(\"Score\", FloatType())\n",
    "              ])\n",
    "\n",
    "data_list = [\n",
    "  (1, 1000, \"Apple\", 0.76),\n",
    "  (2, 1000, \"Apple\", 0.11),\n",
    "  (1, 2000, \"Orange\", 0.98),\n",
    "  (1, 3000, \"Banana\", 0.24),\n",
    "  (2, 3000, \"Banana\", 0.99)    \n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data_list, schema=data_schema) \n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------+\n",
      "|UserKey|Collection                                                       |\n",
      "+-------+-----------------------------------------------------------------+\n",
      "|1      |[[0.98, 2000, Orange], [0.76, 1000, Apple], [0.24, 3000, Banana]]|\n",
      "|2      |[[0.99, 3000, Banana], [0.11, 1000, Apple]]                      |\n",
      "+-------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "data_df.groupBy(\"UserKey\")\n",
    "  .agg(F.sort_array(F.collect_list(F.struct(\"Score\", \"ItemKey\", \"ItemName\")), False))\n",
    "  .toDF(\"UserKey\", \"Collection\")\n",
    "  .show(20, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 - windowSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+\n",
      "|   name|department|          score|\n",
      "+-------+----------+---------------+\n",
      "|    Ali|         0|          [100]|\n",
      "|Barbara|         1|[300, 250, 100]|\n",
      "|  Cesar|         1|     [350, 100]|\n",
      "|Dongmei|         1|     [400, 100]|\n",
      "|    Eli|         2|          [250]|\n",
      "|Florita|         2|[500, 300, 100]|\n",
      "| Gatimu|         3|     [300, 100]|\n",
      "+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_schema = StructType([\n",
    "                  StructField(\"name\", StringType())\n",
    "                 ,StructField(\"department\", IntegerType())\n",
    "                 ,StructField(\"score\", ArrayType(IntegerType()))\n",
    "              ])\n",
    "\n",
    "people_list = [\n",
    "    (\"Ali\", 0, [100]),\n",
    "    (\"Barbara\", 1, [300, 250, 100]),\n",
    "    (\"Cesar\", 1, [350, 100]),\n",
    "    (\"Dongmei\", 1, [400, 100]),\n",
    "    (\"Eli\", 2, [250]),\n",
    "    (\"Florita\", 2, [500, 300, 100]),\n",
    "    (\"Gatimu\", 3, [300, 100])\n",
    "]\n",
    "\n",
    "\n",
    "people_df = spark.createDataFrame(people_list, schema=people_schema) \n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import explode, dense_rank, max\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.col(\"score\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+-------+\n",
      "|department|   name|score|rank|highest|\n",
      "+----------+-------+-----+----+-------+\n",
      "|         1|Dongmei|  400|   1|    400|\n",
      "|         1|  Cesar|  350|   2|    400|\n",
      "|         1|Barbara|  300|   3|    400|\n",
      "|         1|Barbara|  250|   4|    400|\n",
      "|         1|Barbara|  100|   5|    400|\n",
      "|         1|  Cesar|  100|   5|    400|\n",
      "|         1|Dongmei|  100|   5|    400|\n",
      "|         3| Gatimu|  300|   1|    300|\n",
      "|         3| Gatimu|  100|   2|    300|\n",
      "|         2|Florita|  500|   1|    500|\n",
      "|         2|Florita|  300|   2|    500|\n",
      "|         2|    Eli|  250|   3|    500|\n",
      "|         2|Florita|  100|   4|    500|\n",
      "|         0|    Ali|  100|   1|    100|\n",
      "+----------+-------+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at intermediate result\n",
    "(\n",
    "people_df\n",
    "  .withColumn(\"score\", explode(col(\"score\")))\n",
    "  .select(\n",
    "    col(\"department\"),\n",
    "    col(\"name\"),\n",
    "    col(\"score\"),\n",
    "    dense_rank().over(windowSpec).alias(\"rank\"),\n",
    "    max(col(\"score\")).over(windowSpec).alias(\"highest\")\n",
    "  )\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|department|   name|highest|\n",
      "+----------+-------+-------+\n",
      "|         0|    Ali|    100|\n",
      "|         1|Dongmei|    400|\n",
      "|         2|Florita|    500|\n",
      "|         3| Gatimu|    300|\n",
      "+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "people_df\n",
    "  .withColumn(\"score\", explode(col(\"score\")))\n",
    "  .select(\n",
    "    col(\"department\"),\n",
    "    col(\"name\"),\n",
    "    dense_rank().over(windowSpec).alias(\"rank\"),\n",
    "    max(col(\"score\")).over(windowSpec).alias(\"highest\")\n",
    "  )\n",
    "  .where(col(\"rank\") == 1)\n",
    "  .drop(\"rank\")\n",
    "  .orderBy(\"department\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
