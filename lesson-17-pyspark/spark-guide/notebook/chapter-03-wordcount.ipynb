{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"word-count\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = spark.read.text(\"spark.README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|value                                                       |\n",
      "+------------------------------------------------------------+\n",
      "|[Databricks Sandbox](https://community.cloud.databricks.com)|\n",
      "|                                                            |\n",
      "|2019-07-30                                                  |\n",
      "|                                                            |\n",
      "|reinstall Anaconda                                          |\n",
      "+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textFile.select(\"value\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark = textFile.filter(F.lower(F.col(\"value\")).contains(\"spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+\n",
      "|value                                               |\n",
      "+----------------------------------------------------+\n",
      "|Apache Spark                                        |\n",
      "|http://spark.apache.org/                            |\n",
      "|$ mkdir -p ~/spark                                  |\n",
      "|$ cd spark                                          |\n",
      "|$ tar zxvf ~/Downloads/spark-2.4.3-bin-hadoop2.7.tgz|\n",
      "+----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = (\n",
    "    textFile\n",
    "    .select(F.explode(F.split(textFile.value, \"\\s+\")).alias(\"word\"))\n",
    "    .groupBy(\"word\").count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|word   |count|\n",
      "+-------+-----+\n",
      "|       |63   |\n",
      "|$      |21   |\n",
      "|>>>    |14   |\n",
      "|pyspark|8    |\n",
      "|```    |8    |\n",
      "|#      |7    |\n",
      "|python |7    |\n",
      "|Spark  |6    |\n",
      "|##     |6    |\n",
      "|in     |6    |\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCounts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.coalesce(2).write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/wc_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.createOrReplaceTempView(\"textFile\")   # create a table \"textFile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[Databricks Sandb...|\n",
      "|                    |\n",
      "|          2019-07-30|\n",
      "|                    |\n",
      "|  reinstall Anaconda|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from textFile limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|       |   85|\n",
      "|      $|   21|\n",
      "|    >>>|   14|\n",
      "|pyspark|    8|\n",
      "|    ```|    8|\n",
      "|      #|    7|\n",
      "| python|    7|\n",
      "|     in|    6|\n",
      "|   with|    6|\n",
      "|     ##|    6|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_stmt = \"\"\"\n",
    "    with words as (\n",
    "        select \n",
    "            explode(split(value, \" \")) as word \n",
    "        from textFile\n",
    "    )\n",
    "    select \n",
    "        word, count(*) as count\n",
    "    from words\n",
    "    group by word\n",
    "    order by count desc\n",
    "    limit 10\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_stmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sc.textFile(\"spark.README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Databricks Sandbox](https://community.cloud.databricks.com)',\n",
       " '',\n",
       " '2019-07-30',\n",
       " '',\n",
       " 'reinstall Anaconda']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = (\n",
    "    f.flatMap(lambda x: x.split(\" \"))\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[Databricks', 1),\n",
       " ('Sandbox](https://community.cloud.databricks.com)', 1),\n",
       " ('', 85),\n",
       " ('2019-07-30', 1),\n",
       " ('https://www.digitalocean.com/community/tutorials/how-to-install-anaconda-on-ubuntu-18-04-quickstart',\n",
       "  2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 85),\n",
       " ('$', 21),\n",
       " ('>>>', 14),\n",
       " ('```', 8),\n",
       " ('pyspark', 8),\n",
       " ('python', 7),\n",
       " ('#', 7),\n",
       " ('Spark', 6),\n",
       " ('in', 6),\n",
       " ('##', 6)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wc.collect(), key = lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.saveAsTextFile(\"/tmp/wc_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wc_df.csv:\r\n",
      "part-00000-bad9b944-c0e4-4386-ab79-202cd626fad0-c000.csv  _SUCCESS\r\n",
      "part-00001-bad9b944-c0e4-4386-ab79-202cd626fad0-c000.csv\r\n",
      "\r\n",
      "/tmp/wc_rdd.txt:\r\n",
      "part-00000  part-00001\t_SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/wc_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
