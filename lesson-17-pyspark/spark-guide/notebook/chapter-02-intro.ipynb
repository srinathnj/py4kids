{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark: The Definitive Guide\n",
    "\n",
    "\n",
    "Notebooks here are created from book's [Code](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/code) and [Data](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data)\n",
    "\n",
    "After cloning the [Git repo](https://github.com/databricks/Spark-The-Definitive-Guide) locally, set os env var `SPARK_BOOK_DATA_PATH` to that folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Get Started\n",
    "\n",
    "#### Databrick Cloud Sandbox\n",
    "\n",
    "Use Spark Cluster free at :\n",
    "\n",
    "https://community.cloud.databricks.com/\n",
    "\n",
    "#### How to run code example\n",
    "\n",
    "https://github.com/databricks/Spark-The-Definitive-Guide\n",
    "\n",
    "dataset can be found at \n",
    "```\n",
    "%fs ls /databricks-datasets/definitive-guide/data\n",
    "```\n",
    "\n",
    "imported py code to Databricks workspace : `HOME > Spark_Guide > py` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Installation\n",
    "\n",
    "To install pyspark\n",
    "```\n",
    "$ pip install pyspark\n",
    "```\n",
    "\n",
    "To start jupyter notebook\n",
    "```\n",
    "$ PYSPARK_DRIVER_PYTHON=\"jupyter\" PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark\n",
    "```\n",
    "\n",
    "To use other pyspark packages, add `--packages <pkg-name>`, e.g.\n",
    "```\n",
    "$ PYSPARK_DRIVER_PYTHON=\"jupyter\" PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark --packages graphframes:graphframes:0.8.0-spark3.0-s_2.12\n",
    "```\n",
    "pkg jar is at `$SPARK_HOME/jars/graphframes-0.8.0-spark3.0-s_2.12.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Scala console\n",
    "\n",
    "```\n",
    "cmd> $SPARK_HOME/bin/spark-shell\n",
    "```\n",
    "\n",
    "Launch pyspark console\n",
    "\n",
    "```\n",
    "cmd> $SPARK_HOME/bin/pyspark\n",
    "```\n",
    "\n",
    "Launch SQL console\n",
    "\n",
    "```\n",
    "cmd> $SPARK_HOME/bin/spark-sql\n",
    "```\n",
    "\n",
    "Submit Spark app\n",
    "\n",
    "```\n",
    "cmd> $SPARK_HOME/bin/spark-submit \\\n",
    "    --master local[*] \\\n",
    "    --packages 'com.somesparkjar.dependency:1.0.0' \\\n",
    "    --py-files packages.zip \\\n",
    "    --files configs/etl_config.json \\\n",
    "    jobs/etl_job.py  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection of notebooks have been tested using \n",
    "- python.version: 3.8.5\n",
    "- spark.version: spark-3.0.1-bin-hadoop2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to upload file to databrick cloud\n",
    "\n",
    "https://forums.databricks.com/questions/10648/upload-local-files-into-dbfs-1.html\n",
    "\n",
    "\n",
    "git clone https://github.com/databricks/Spark-The-Definitive-Guide\n",
    "\n",
    "replace data path inside `code` folder with `/databricks-datasets/definitive-guide/data` globally\n",
    "\n",
    "```\n",
    "$ zip -r spark_guide_code.dbc spark_guide_code/\n",
    "```\n",
    "\n",
    "login to https://community.cloud.databricks.com/\n",
    "\n",
    "create a cluster\n",
    "\n",
    "workspace > Import File\n",
    "\n",
    "Not working because file format are different!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add `export SPARK_BOOK_DATA_PATH=~/spark_data/` to ~/.bash_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_BOOK_DATA_PATH = os.environ['SPARK_BOOK_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wengong/spark_data/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPARK_BOOK_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"chapter-02-intro\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter-02-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f54cabff2e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter-02-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=chapter-02-intro>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'chapter-02-intro'),\n",
       " ('spark.driver.host', '192.168.0.114'),\n",
       " ('spark.app.id', 'local-1622772553118'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/home/wengong/spark/spark-3.0.1-bin-hadoop2.7/jars/sqlite-jdbc-3.27.2.1.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', '/tmp/hive/spark-warehouse'),\n",
       " ('spark.executor.extraClassPath',\n",
       "  '/home/wengong/spark/spark-3.0.1-bin-hadoop2.7/jars/sqlite-jdbc-3.27.2.1.jar'),\n",
       " ('spark.driver.port', '39273'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = sc.getConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple dataframe of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.range(1000) returns a RDD, toDF() converts it to DataFrame\n",
    "myRange = spark.range(10).toDF(\"number\")\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "evens = divisBy2.collect()     # convert to python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(number=0), Row(number=2), Row(number=4), Row(number=6), Row(number=8)],\n",
       " 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evens, evens[0].number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert collection to RDD\n",
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "nums = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "test_rdd = sc.parallelize([Row(1), Row(2), Row(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(1)>, <Row(2)>, <Row(3)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.toDF(\"id\").show()   # name column \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work with data in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "file_path = SPARK_BOOK_DATA_PATH + \"/data/flight-data/csv/2015-summary.csv\"\n",
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short form:\n",
    "\n",
    "`flightData2015 = spark.read.csv(file_path, header=True, inferSchema=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to parquet file\n",
    "file_path = SPARK_BOOK_DATA_PATH + \"/data/flight-data/parquet/2015-summary.parquet\"\n",
    "flightData2015.coalesce(1).write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wengong/spark_data//data/flight-data/parquet/2015-summary.parquet'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-d6a476df-f63d-47b3-9dff-d04975413814-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/wengong/spark_data//data/flight-data/parquet/2015-summary.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read it back\n",
    "flightData2015_2 = spark\\\n",
    "    .read\\\n",
    "    .format(\"parquet\")\\\n",
    "    .load(file_path)\n",
    "\n",
    "flightData2015_2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DataFrame to temp Table\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SQL directly against temp table\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, count(*)\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    --having count(1) > 4\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|DEST_COUNTRY_NAME|count(1)|\n",
      "+-----------------+--------+\n",
      "|         Anguilla|       1|\n",
      "|           Russia|       1|\n",
      "|         Paraguay|       1|\n",
      "|          Senegal|       1|\n",
      "|           Sweden|       1|\n",
      "+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay = flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|         Anguilla|    1|\n",
      "|           Russia|    1|\n",
      "|         Paraguay|    1|\n",
      "|          Senegal|    1|\n",
      "|           Sweden|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Catalyst turns logic plans to optimized physical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#41, 200), true, [id=#191]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#41] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/wengong/spark_data/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#41, 200), true, [id=#210]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#41] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/wengong/spark_data/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying physical plans are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(F.max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = spark.sql(\"\"\"\n",
    "    SELECT max(count) as max_count\n",
    "    FROM flight_data_2015\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max_count=370002)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_count.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370002"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_count.collect()[0].max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    ORDER BY sum(count) DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import desc\n",
    "\n",
    "top5_destDF = flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(F.desc(\"destination_total\"))\\\n",
    "  .limit(5)\n",
    "\n",
    "top5_destDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#169L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#41,destination_total#169L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[sum(cast(count#43 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#41, 200), true, [id=#352]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[partial_sum(cast(count#43 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#41,count#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/wengong/spark_data/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_destDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#145L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#41,destination_total#143L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[sum(cast(count#43 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#41, 200), true, [id=#376]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#41], functions=[partial_sum(cast(count#43 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#41,count#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/wengong/spark_data/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run SQL on files directly\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#manually-specifying-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = SPARK_BOOK_DATA_PATH + \"/data/flight-data/parquet/2015-summary.parquet\"\n",
    "df = spark.sql(f\"SELECT * FROM parquet.`{file_path}` order by count desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|     United States|      United States|370002|\n",
      "|     United States|             Canada|  8483|\n",
      "|            Canada|      United States|  8399|\n",
      "|     United States|             Mexico|  7187|\n",
      "|            Mexico|      United States|  7140|\n",
      "|    United Kingdom|      United States|  2025|\n",
      "|     United States|     United Kingdom|  1970|\n",
      "|             Japan|      United States|  1548|\n",
      "|     United States|              Japan|  1496|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|     United States|            Germany|  1336|\n",
      "|       South Korea|      United States|  1048|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|     United States|             France|   952|\n",
      "|            France|      United States|   935|\n",
      "|     United States|              China|   920|\n",
      "|          Colombia|      United States|   873|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
