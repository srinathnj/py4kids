{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Spark Developer Certification - Comprehensive Study Guide](https://github.com/mdrakiburrahman/databricks-certification)\n",
    "\n",
    "~/spark/databrick-cert/databricks-certification/Comprehensive_study_guide_for_Spark_Developer_Certification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips and Tricks\n",
    "\n",
    "#### Driver\n",
    "\n",
    "The driver is the machine in which the application runs (master node). It is responsible for three main things: \n",
    "* Maintaining information about the Spark Application, \n",
    "* Responding to the userâ€™s program, \n",
    "* Analyzing, distributing, and scheduling work across the executors.\n",
    "\n",
    "Worker nodes performs computation in parallel.\n",
    "\n",
    "\n",
    "#### Dynamic Partition Pruning (DPP) \n",
    "\n",
    "DPP can auto-optimize your queries and make them more performant automatically.\n",
    "enabling it via property `spark.sql.optimizer.dynamicPartitionPruning.enabled`\n",
    "\n",
    "#### SkewJoin\n",
    "\n",
    "Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions. enabling it via property `spark.sql.adaptive.skewJoin.enabled`\n",
    "\n",
    "#### cache\n",
    "\n",
    "When you use cache() or persist(), the DataFrame is not fully cached until you invoke an action that goes through every record (e.g., count()). If you use an action like take(1), only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record.\n",
    "\n",
    "\n",
    "When using DataFrame.persist() data on disk is always serialized.\n",
    "\n",
    "#### broadcast\n",
    "\n",
    "By default `spark.sql.autoBroadcastJoinThreshold = 10MB`, any value above this threashold will not force a broadcast join\n",
    "\n",
    "#### dataframe sort\n",
    "\n",
    "`df.orderBy(desc_nulls_first(\"a\"))` will sort a df with column \"a\" containing null and keep null first. (similarly `desc_nulls_last(\"a\")`).  \n",
    "* `df.sort(*cols, **kwargs)` is another sort API.\n",
    "* more examples - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sort.html#pyspark.sql.DataFrame.sort\n",
    "\n",
    "#### join-expression\n",
    "pyspark uses `==` but scalar uses `===`\n",
    "\n",
    "#### JVM garbage collection\n",
    "remember that the cost of garbage collection is proportional to the number of Java objects. To reduce `gc`, \n",
    "* create fewer objects\n",
    "* increase java heap space size\n",
    "* persist objects in serialized form\n",
    "\n",
    "#### global external/unmanaged table\n",
    "\n",
    "Spark manages the metadata, while you control the data location. As soon as you add `path` option in dataframe writer it will be treated as global external/unmanaged table. When you drop table only metadata gets dropped. A global unmanaged/external table is available across all clusters.\n",
    "\n",
    "When defining a table from files on disk, you create an unmanaged table.\n",
    "When you use `saveAsTable` on a dataframe, you create a managed table for which Spark with track both metadata and data.\n",
    "\n",
    "#### execution mode vs deployment mode\n",
    "\n",
    "3 execution modes:\n",
    "* cluster\n",
    "* client\n",
    "* local\n",
    "\n",
    "4 deployment modes:\n",
    "* local - driver/executors in one machine (non-cluster mode)\n",
    "* standalone - spark cluster runs only spark apps \n",
    "* YARN - spark app co-exists with other non-spark JVM apps\n",
    "* Meso - schedule JVM, C++, python apps, manage CPU/Net resources plus Memory\n",
    "\n",
    "#### Hive data_type to define Spark DataFrame schema\n",
    "- https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types\n",
    "\n",
    "* INT, SMALLINT, TINYINT, BIGINT\n",
    "* FLOAT, DOUBLE, DECIMAL, NUMERIC\n",
    "* DATE, TIMESTAMP, INTERVAL\n",
    "* STRING, VARCHAR, CHAR\n",
    "* BOOLEAN\n",
    "* BINARY\n",
    "\n",
    "\n",
    "#### Review Spark conf settings\n",
    "https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "- Spark properties control most application parameters and can be set by using a SparkConf object, or through Java system properties.\n",
    "- Environment variables can be used to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node.\n",
    "- Logging can be configured through log4j.properties.\n",
    "    \n",
    "    \n",
    "#### Cluster Overview\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html\n",
    "\n",
    "#### Repartition vs Coalesce\n",
    "https://ashwin.cloud/blog/spark-repartition-vs-coalesce/\n",
    "\n",
    "Repartition can be used for increasing or decreasing the number of partitions. Whereas Coalesce can only be used for decreasing the number of partitions. Coalesce is a less expensive operation than Repartition as Coalesce reduces data movement between the nodes while Repartition shuffles all data over the network.\n",
    "\n",
    "\n",
    "#### Structured streaming guide\n",
    "https://spark.apache.org/docs/3.1.1/structured-streaming-programming-guide.html#schema-inference-and-partition-of-streaming-dataframesdatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import *\n",
    "from pyspark import StorageLevel\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.114:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7a94ca50a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = spark.createDataFrame([1, 2, 3, 4], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"name\",\"gender\") # Define the Row `Employee' with one column/key\n",
    "employee1 = Employee('Bob',\"M\") # Define against the Row 'Employee'\n",
    "employee2 = Employee('Sam',\"M\") # Define against the Row 'Employee'\n",
    "employee3 = Employee('Jane',\"F\") # Define against the Row 'Employee'\n",
    "\n",
    "# Create the Departments\n",
    "Department = Row(\"name\", \"department\") # Define the Row `Department' with two columns/keys\n",
    "department1 = Department('Bob', 'Accounts') # Define against the Row 'Department'\n",
    "department2 = Department('Alice', 'Sales') # Define against the Row 'Department'\n",
    "department3 = Department('Sam', 'HR') # Define against the Row 'Department'\n",
    "\n",
    "# Create DataFrames from rows\n",
    "employeeDF = spark.createDataFrame([employee1, employee2]) \n",
    "departmentDF = spark.createDataFrame([department1, department2, department3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = employeeDF[\"name\"] == departmentDF[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----------+\n",
      "|name|gender|name|department|\n",
      "+----+------+----+----------+\n",
      "| Bob|     M| Bob|  Accounts|\n",
      "| Sam|     M| Sam|        HR|\n",
      "+----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----------+\n",
      "|name|gender|name|department|\n",
      "+----+------+----+----------+\n",
      "| Bob|     M| Bob|  Accounts|\n",
      "| Sam|     M| Sam|        HR|\n",
      "+----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|gender|\n",
      "+----+------+\n",
      "| Bob|     M|\n",
      "| Sam|     M|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|gender|\n",
      "+----+------+\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+----------+\n",
      "|name|gender| name|department|\n",
      "+----+------+-----+----------+\n",
      "| Bob|     M|  Bob|  Accounts|\n",
      "| Sam|     M|  Sam|        HR|\n",
      "|null|  null|Alice|     Sales|\n",
      "+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+----------+\n",
      "|name|gender|name|department|\n",
      "+----+------+----+----------+\n",
      "| Bob|     M| Bob|  Accounts|\n",
      "| Sam|     M| Sam|        HR|\n",
      "+----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"cross\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+----------+\n",
      "|name|gender| name|department|\n",
      "+----+------+-----+----------+\n",
      "| Bob|     M|  Bob|  Accounts|\n",
      "| Sam|     M|  Sam|        HR|\n",
      "|null|  null|Alice|     Sales|\n",
      "+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+----------+\n",
      "|name|gender| name|department|\n",
      "+----+------+-----+----------+\n",
      "| Bob|     M|  Bob|  Accounts|\n",
      "| Sam|     M|  Sam|        HR|\n",
      "|null|  null|Alice|     Sales|\n",
      "+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.join(departmentDF, joinExpression, how=\"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|letter|position|\n",
      "+------+--------+\n",
      "|     A|       1|\n",
      "|     B|       2|\n",
      "|     C|       3|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"letter\", StringType(), True),\n",
    "        StructField(\"position\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter', 'position']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(letter,StringType,true),StructField(position,IntegerType,true),StructField(value,IntegerType,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.add(\"value\",IntegerType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter', 'position', 'value']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|letter|position|\n",
      "+------+--------+\n",
      "|     A|       1|\n",
      "|     B|       2|\n",
      "|     C|       3|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|letter|position|value|\n",
      "+------+--------+-----+\n",
      "|     A|       1|  100|\n",
      "|     B|       2|  100|\n",
      "|     C|       3|  100|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"value\", lit(100))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter', 'position', 'value']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letter', 'position', 'value']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new rows with union\n",
    "df = df.union(spark.createDataFrame(data=[[\"D\", 4, 1000]], schema=df.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|letter|position|value|\n",
      "+------+--------+-----+\n",
      "|     A|       1|  100|\n",
      "|     B|       2|  100|\n",
      "|     C|       3|  100|\n",
      "|     D|       4| 1000|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(letter='B', sum(value)=100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"letter\").sum(\"value\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|count(letter)|sum(value)|\n",
      "+-------------+----------+\n",
      "|            3|       300|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"value\":\"sum\", \"letter\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "Department = Row(\"id\", \"name\")\n",
    "department1 = Department('123456', 'Computer Science')\n",
    "department2 = Department('789012', 'Mechanical Engineering')\n",
    "department3 = Department('345678', 'Theater and Drama')\n",
    "department4 = Department('901234', 'Indoor Recreation')\n",
    "department5 = Department('000000', 'All Students')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "DepartmentWithEmployees = Row(\"department\", \"employees\")\n",
    "departmentWithEmployees1 = DepartmentWithEmployees(department1, [employee1, employee2])\n",
    "departmentWithEmployees2 = DepartmentWithEmployees(department2, [employee3, employee4])\n",
    "departmentWithEmployees3 = DepartmentWithEmployees(department3, [employee5, employee4])\n",
    "departmentWithEmployees4 = DepartmentWithEmployees(department4, [employee2, employee3])\n",
    "departmentWithEmployees5 = DepartmentWithEmployees(department5, [employee1, employee2, employee3, employee4, employee5])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|department                      |employees                                                                                                                                                                                                                                 |\n",
      "+--------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[123456, Computer Science]      |[[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000]]                                                                                                                                     |\n",
      "|[789012, Mechanical Engineering]|[[matei,, no-reply@waterloo.edu, 140000], [, wendell, no-reply@berkeley.edu, 160000]]                                                                                                                                                     |\n",
      "|[345678, Theater and Drama]     |[[michael, jackson, no-reply@neverla.nd, 80000], [, wendell, no-reply@berkeley.edu, 160000]]                                                                                                                                              |\n",
      "|[901234, Indoor Recreation]     |[[xiangrui, meng, no-reply@stanford.edu, 120000], [matei,, no-reply@waterloo.edu, 140000]]                                                                                                                                                |\n",
      "|[000000, All Students]          |[[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000], [matei,, no-reply@waterloo.edu, 140000], [, wendell, no-reply@berkeley.edu, 160000], [michael, jackson, no-reply@neverla.nd, 80000]]|\n",
      "+--------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2, departmentWithEmployees3, departmentWithEmployees4, departmentWithEmployees5]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " department | [123456, Computer Science]                                                                                                                                                                                                                 \n",
      " employees  | [[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000]]                                                                                                                                      \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " department | [789012, Mechanical Engineering]                                                                                                                                                                                                           \n",
      " employees  | [[matei,, no-reply@waterloo.edu, 140000], [, wendell, no-reply@berkeley.edu, 160000]]                                                                                                                                                      \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " department | [345678, Theater and Drama]                                                                                                                                                                                                                \n",
      " employees  | [[michael, jackson, no-reply@neverla.nd, 80000], [, wendell, no-reply@berkeley.edu, 160000]]                                                                                                                                               \n",
      "-RECORD 3------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " department | [901234, Indoor Recreation]                                                                                                                                                                                                                \n",
      " employees  | [[xiangrui, meng, no-reply@stanford.edu, 120000], [matei,, no-reply@waterloo.edu, 140000]]                                                                                                                                                 \n",
      "-RECORD 4------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " department | [000000, All Students]                                                                                                                                                                                                                     \n",
      " employees  | [[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000], [matei,, no-reply@waterloo.edu, 140000], [, wendell, no-reply@berkeley.edu, 160000], [michael, jackson, no-reply@neverla.nd, 80000]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "|     5|\n",
      "|     7|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1,8,2).toDF(\"number\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample question for certification\n",
    "\n",
    "How to create spark dataframe from list\n",
    "\n",
    "https://stackoverflow.com/questions/43444925/how-to-create-dataframe-from-list-in-spark-sql/50969995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----------+\n",
      "| Name|Age|Score|       DOB|\n",
      "+-----+---+-----+----------+\n",
      "|Alice| 20|700.5|2001-10-02|\n",
      "|  Bob| 15|500.5|2006-01-01|\n",
      "+-----+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "                StructField(\"Name\", StringType())\n",
    "               ,StructField(\"Age\", IntegerType())\n",
    "               ,StructField(\"Score\", DoubleType())\n",
    "               ,StructField(\"DOB\", StringType())\n",
    "              ])\n",
    "\n",
    "# using HIVE DDL\n",
    "# schema = \"Name STRING, Age INT, Score DOUBLE, DOB VARCHAR(10)\" \n",
    "\n",
    "data = [\n",
    "    ('Alice', 20, 700.50, \"2001-10-02\"), \n",
    "    ('Bob', 15, 500.50, \"2006-01-01\"), \n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data, schema=schema) \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1002, 3001, 4002, 2003, 2002, 3004, 1003, 4006]\n",
    "b = (spark\n",
    "  .createDataFrame(list(map(lambda x: Row(value=x), a)))\n",
    "  .withColumn(\"x\", col(\"value\") % 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value|  x|\n",
      "+-----+---+\n",
      "| 3001|  1|\n",
      "| 4002|  2|\n",
      "| 1002|  2|\n",
      "| 2002|  2|\n",
      "| 1003|  3|\n",
      "| 2003|  3|\n",
      "| 3004|  4|\n",
      "| 4006|  6|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.sort(col(\"x\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|count|total|\n",
      "+-----+-----+\n",
      "|    3| 7006|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = (\n",
    "    b.groupBy(col(\"x\"))\n",
    "    .agg(count(\"x\"), sum(\"value\"))\n",
    "    .drop(\"x\")\n",
    "    .toDF(\"count\", \"total\")\n",
    "    .orderBy(col(\"count\").desc(), col(\"total\"))\n",
    "    .limit(1)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----+\n",
      "|UserKey|ItemKey|ItemName|Score|\n",
      "+-------+-------+--------+-----+\n",
      "|      1|   1000|   Apple| 0.76|\n",
      "|      2|   1000|   Apple| 0.11|\n",
      "|      1|   2000|  Orange| 0.98|\n",
      "|      1|   3000|  Banana| 0.24|\n",
      "|      2|   3000|  Banana| 0.99|\n",
      "+-------+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"UserKey\", IntegerType())\n",
    "        ,StructField(\"ItemKey\", IntegerType())\n",
    "        ,StructField(\"ItemName\", StringType())\n",
    "        ,StructField(\"Score\", FloatType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_list = [\n",
    "  (1, 1000, \"Apple\", 0.76),\n",
    "  (2, 1000, \"Apple\", 0.11),\n",
    "  (1, 2000, \"Orange\", 0.98),\n",
    "  (1, 3000, \"Banana\", 0.24),\n",
    "  (2, 3000, \"Banana\", 0.99)    \n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data_list, schema=data_schema) \n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------+\n",
      "|UserKey|Collection                                                       |\n",
      "+-------+-----------------------------------------------------------------+\n",
      "|1      |[[0.98, 2000, Orange], [0.76, 1000, Apple], [0.24, 3000, Banana]]|\n",
      "|2      |[[0.99, 3000, Banana], [0.11, 1000, Apple]]                      |\n",
      "+-------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "data_df.groupBy(\"UserKey\")\n",
    "  .agg(sort_array(collect_list(struct(\"Score\", \"ItemKey\", \"ItemName\")), asc=False))\n",
    "  .toDF(\"UserKey\", \"Collection\")\n",
    "  .show(20, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+\n",
      "|   name|department|          score|\n",
      "+-------+----------+---------------+\n",
      "|    Ali|         0|          [100]|\n",
      "|Barbara|         1|[300, 250, 100]|\n",
      "|  Cesar|         1|     [350, 100]|\n",
      "|Dongmei|         1|     [400, 100]|\n",
      "|    Eli|         2|          [250]|\n",
      "|Florita|         2|[500, 300, 100]|\n",
      "| Gatimu|         3|     [300, 100]|\n",
      "+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_schema = StructType([\n",
    "                  StructField(\"name\", StringType())\n",
    "                 ,StructField(\"department\", IntegerType())\n",
    "                 ,StructField(\"score\", ArrayType(IntegerType()))\n",
    "              ])\n",
    "\n",
    "people_list = [\n",
    "    (\"Ali\", 0, [100]),\n",
    "    (\"Barbara\", 1, [300, 250, 100]),\n",
    "    (\"Cesar\", 1, [350, 100]),\n",
    "    (\"Dongmei\", 1, [400, 100]),\n",
    "    (\"Eli\", 2, [250]),\n",
    "    (\"Florita\", 2, [500, 300, 100]),\n",
    "    (\"Gatimu\", 3, [300, 100])\n",
    "]\n",
    "\n",
    "\n",
    "people_df = spark.createDataFrame(people_list, schema=people_schema) \n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import explode, dense_rank, max\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"score\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+-------+\n",
      "|department|   name|score|rank|highest|\n",
      "+----------+-------+-----+----+-------+\n",
      "|         1|Dongmei|  400|   1|    400|\n",
      "|         1|  Cesar|  350|   2|    400|\n",
      "|         1|Barbara|  300|   3|    400|\n",
      "|         1|Barbara|  250|   4|    400|\n",
      "|         1|Barbara|  100|   5|    400|\n",
      "|         1|  Cesar|  100|   5|    400|\n",
      "|         1|Dongmei|  100|   5|    400|\n",
      "|         3| Gatimu|  300|   1|    300|\n",
      "|         3| Gatimu|  100|   2|    300|\n",
      "|         2|Florita|  500|   1|    500|\n",
      "|         2|Florita|  300|   2|    500|\n",
      "|         2|    Eli|  250|   3|    500|\n",
      "|         2|Florita|  100|   4|    500|\n",
      "|         0|    Ali|  100|   1|    100|\n",
      "+----------+-------+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at intermediate result\n",
    "(\n",
    "people_df\n",
    "  .withColumn(\"score\", explode(col(\"score\")))\n",
    "  .select(\n",
    "    col(\"department\"),\n",
    "    col(\"name\"),\n",
    "    col(\"score\"),\n",
    "    dense_rank().over(windowSpec).alias(\"rank\"),\n",
    "    max(col(\"score\")).over(windowSpec).alias(\"highest\")\n",
    "  )\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# look at intermediate result\n",
    "# use rank()\n",
    "(\n",
    "people_df\n",
    "  .withColumn(\"score\", explode(col(\"score\")))\n",
    "  .select(\n",
    "    col(\"department\"),\n",
    "    col(\"name\"),\n",
    "    col(\"score\"),\n",
    "    rank().over(windowSpec).alias(\"rank\"),\n",
    "    max(col(\"score\")).over(windowSpec).alias(\"highest\")\n",
    "  )\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|department|   name|highest|\n",
      "+----------+-------+-------+\n",
      "|         0|    Ali|    100|\n",
      "|         1|Dongmei|    400|\n",
      "|         2|Florita|    500|\n",
      "|         3| Gatimu|    300|\n",
      "+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "people_df\n",
    "  .withColumn(\"score\", explode(col(\"score\")))\n",
    "  .select(\n",
    "    col(\"department\"),\n",
    "    col(\"name\"),\n",
    "    dense_rank().over(windowSpec).alias(\"rank\"),\n",
    "    max(col(\"score\")).over(windowSpec).alias(\"highest\")\n",
    "  )\n",
    "  .where(col(\"rank\") == 1)\n",
    "  .drop(\"rank\")\n",
    "  .orderBy(\"department\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
