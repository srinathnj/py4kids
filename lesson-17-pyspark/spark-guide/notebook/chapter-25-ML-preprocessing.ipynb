{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"chapter-25-ML-preprocessing\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "import os\n",
    "SPARK_BOOK_DATA_PATH = os.environ['SPARK_BOOK_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(SPARK_BOOK_DATA_PATH + \"/data/retail-data/by-day/*.csv\")\\\n",
    "  .coalesce(5)\\\n",
    "  .where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|580538   |23084    |RABBIT NIGHT LIGHT             |48      |2011-12-05 08:38:00|1.79     |14075.0   |United Kingdom|\n",
      "|580538   |23077    |DOUGHNUT LIP GLOSS             |20      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|\n",
      "|580538   |22906    |12 MESSAGE CARDS WITH ENVELOPES|24      |2011-12-05 08:38:00|1.65     |14075.0   |United Kingdom|\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeIntDF = spark.read.parquet(SPARK_BOOK_DATA_PATH + \"/data/simple-ml-integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|4   |5   |6   |\n",
      "|1   |2   |3   |\n",
      "|7   |8   |9   |\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleDF = spark.read.json(SPARK_BOOK_DATA_PATH + \"/data/simple-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color|lab |value1|value2            |\n",
      "+-----+----+------+------------------+\n",
      "|green|good|1     |14.386294994851129|\n",
      "|blue |bad |8     |14.386294994851129|\n",
      "|blue |bad |12    |14.386294994851129|\n",
      "|green|good|15    |38.97187133755819 |\n",
      "|green|good|12    |14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleDF = spark.read.parquet(SPARK_BOOK_DATA_PATH + \"/data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|id |features      |\n",
      "+---+--------------+\n",
      "|0  |[1.0,0.1,-1.0]|\n",
      "|1  |[2.0,1.1,1.0] |\n",
      "|0  |[1.0,0.1,-1.0]|\n",
      "|1  |[2.0,1.1,1.0] |\n",
      "|1  |[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                                                              |label|\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n",
      "|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n",
      "|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|119          |62      |14452.0   |\n",
      "|440          |143     |16916.0   |\n",
      "|630          |72      |17633.0   |\n",
      "|34           |6       |14768.0   |\n",
      "|1542         |30      |13094.0   |\n",
      "+-------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQLTransformer - like spark.sql()\n",
    "\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "  .setStatement(\"\"\"\n",
    "    SELECT sum(Quantity), count(*), CustomerID\n",
    "    FROM __THIS__\n",
    "    GROUP BY CustomerID\n",
    "  \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|119          |62      |14452.0   |\n",
      "|440          |143     |16916.0   |\n",
      "|630          |72      |17633.0   |\n",
      "|34           |6       |14768.0   |\n",
      "|1542         |30      |13094.0   |\n",
      "+-------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sum(Quantity), count(*), CustomerID from sales group by CustomerID\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_91f1042ca8cb__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|4   |5   |6   |[4.0,5.0,6.0]                       |\n",
      "|1   |2   |3   |[1.0,2.0,3.0]                       |\n",
      "|7   |8   |9   |[7.0,8.0,9.0]                       |\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VectorAssembler - transformer to assemble columns into vector\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|id |\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|id  |Bucketizer_bffc8656d38c__output|\n",
      "+----+-------------------------------+\n",
      "|0.0 |0.0                            |\n",
      "|1.0 |0.0                            |\n",
      "|2.0 |0.0                            |\n",
      "|3.0 |0.0                            |\n",
      "|4.0 |0.0                            |\n",
      "|5.0 |1.0                            |\n",
      "|6.0 |1.0                            |\n",
      "|7.0 |1.0                            |\n",
      "|8.0 |1.0                            |\n",
      "|9.0 |1.0                            |\n",
      "|10.0|2.0                            |\n",
      "|11.0|2.0                            |\n",
      "|12.0|2.0                            |\n",
      "|13.0|2.0                            |\n",
      "|14.0|2.0                            |\n",
      "|15.0|2.0                            |\n",
      "|16.0|2.0                            |\n",
      "|17.0|2.0                            |\n",
      "|18.0|2.0                            |\n",
      "|19.0|2.0                            |\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bucketizer - transformer to split data into buckets\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileDiscretizer - transformer to partition data by Percentile\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setInputCol(\"id\").setNumBuckets(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+\n",
      "|id  |QuantileDiscretizer_4f6bd0bcc5c0__output|\n",
      "+----+----------------------------------------+\n",
      "|0.0 |0.0                                     |\n",
      "|1.0 |0.0                                     |\n",
      "|2.0 |0.0                                     |\n",
      "|3.0 |1.0                                     |\n",
      "|4.0 |1.0                                     |\n",
      "|5.0 |1.0                                     |\n",
      "|6.0 |1.0                                     |\n",
      "|7.0 |2.0                                     |\n",
      "|8.0 |2.0                                     |\n",
      "|9.0 |2.0                                     |\n",
      "|10.0|2.0                                     |\n",
      "|11.0|3.0                                     |\n",
      "|12.0|3.0                                     |\n",
      "|13.0|3.0                                     |\n",
      "|14.0|3.0                                     |\n",
      "|15.0|4.0                                     |\n",
      "|16.0|4.0                                     |\n",
      "|17.0|4.0                                     |\n",
      "|18.0|4.0                                     |\n",
      "|19.0|4.0                                     |\n",
      "+----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketer.fit(contDF).transform(contDF).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |StandardScaler_3325a6ec77c0__output                         |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|1  |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902]  |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler - transformer to normalize data\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "|id |features      |MinMaxScaler_e07d77613bde__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[5.0,5.0,5.0]                    |\n",
      "|1  |[2.0,1.1,1.0] |[7.5,5.5,7.5]                    |\n",
      "|0  |[1.0,0.1,-1.0]|[5.0,5.0,5.0]                    |\n",
      "|1  |[2.0,1.1,1.0] |[7.5,5.5,7.5]                    |\n",
      "|1  |[3.0,10.1,3.0]|[10.0,10.0,10.0]                 |\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "minMax.fit(scaleDF).transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------------------------+\n",
      "|id |features      |MaxAbsScaler_585c91198f51__output                            |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009903,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009903,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]                                                |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "maScaler.fit(scaleDF).transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "|id |features      |ElementwiseProduct_8d94b9364b23__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[10.0,1.5,-20.0]                       |\n",
      "|1  |[2.0,1.1,1.0] |[20.0,16.5,20.0]                       |\n",
      "|0  |[1.0,0.1,-1.0]|[10.0,1.5,-20.0]                       |\n",
      "|1  |[2.0,1.1,1.0] |[20.0,16.5,20.0]                       |\n",
      "|1  |[3.0,10.1,3.0]|[30.0,151.5,60.0]                      |\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "  .setScalingVec(scaleUpVec)\\\n",
    "  .setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------------+\n",
      "|id |features      |Normalizer_5f82750e3449__output                                |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|1  |[3.0,10.1,3.0]|[0.18633540372670807,0.6273291925465838,0.18633540372670807]   |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color|lab |value1|value2            |labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|1     |14.386294994851129|1.0     |\n",
      "|blue |bad |8     |14.386294994851129|0.0     |\n",
      "|blue |bad |12    |14.386294994851129|0.0     |\n",
      "|green|good|15    |38.97187133755819 |1.0     |\n",
      "|green|good|12    |14.386294994851129|1.0     |\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StringIndexer - transformer to convert categorical data into number\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|color|lab |value1|value2            |labelInd|IndexToString_1e66c232baf4__output|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|green|good|1     |14.386294994851129|1.0     |good                              |\n",
      "|blue |bad |8     |14.386294994851129|0.0     |bad                               |\n",
      "|blue |bad |12    |14.386294994851129|0.0     |bad                               |\n",
      "|green|good|15    |38.97187133755819 |1.0     |good                              |\n",
      "|green|good|12    |14.386294994851129|1.0     |good                              |\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IndexToString - transformer to convert number back to category\n",
    "\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------+\n",
      "|color|lab |value1|value2            |value1_Ind|\n",
      "+-----+----+------+------------------+----------+\n",
      "|green|good|1     |14.386294994851129|0.0       |\n",
      "|blue |bad |8     |14.386294994851129|7.0       |\n",
      "|blue |bad |12    |14.386294994851129|1.0       |\n",
      "|green|good|15    |38.97187133755819 |3.0       |\n",
      "|green|good|12    |14.386294994851129|1.0       |\n",
      "+-----+----+------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"value1_Ind\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|features     |label|idxed        |\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|1    |[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|2    |[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|3    |[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "idxIn = spark.createDataFrame([\n",
    "  (Vectors.dense(1, 2, 3),1),\n",
    "  (Vectors.dense(2, 5, 6),2),\n",
    "  (Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "indxr = VectorIndexer()\\\n",
    "  .setInputCol(\"features\")\\\n",
    "  .setOutputCol(\"idxed\")\\\n",
    "  .setMaxCategories(2)\n",
    "indxr.fit(idxIn).transform(idxIn).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-64f90509c5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolorLab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlblIndxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colorInd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorLab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer - transformer to turn sentence into word array\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, water, transfer, tattoos]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RegexTokenizer - Tokenizer with RegEx\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---------------+\n",
      "|Description                    |DescOut        |\n",
      "+-------------------------------+---------------+\n",
      "|RABBIT NIGHT LIGHT             |[ ,  ]         |\n",
      "|DOUGHNUT LIP GLOSS             |[ ,  ,  ]      |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[ ,  ,  ,  ]   |\n",
      "|BLUE HARMONICA IN BOX          |[ ,  ,  ,  ]   |\n",
      "|GUMBALL COAT RACK              |[ ,  ]         |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[ ,  ,  ,  ,  ]|\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[ ,  ,  ]      |\n",
      "|CAMOUFLAGE LED TORCH           |[ ,  ]         |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[ ,  ,  ,  ,  ]|\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[ ,  ,  ,  ]   |\n",
      "+-------------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setGaps(False)\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |StopWordsRemover_352087335d7d__output|\n",
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|[12, message, cards, envelopes]      |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |[blue, harmonica, box]               |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "+-------------------------------+-------------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "  .setStopWords(englishStopWords)\\\n",
    "  .setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_89b73e4d33d9__output           |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue, harmonica, in, box]           |\n",
      "|[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_09d12ca8503c__output                             |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue harmonica, harmonica in, in box]                 |\n",
      "|[gumball, coat, rack]                |[gumball coat, coat rack]                              |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(5, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+--------------------------------------+\n",
      "|Description                    |DescOut                              |countVec                              |\n",
      "+-------------------------------+-------------------------------------+--------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[150,185,212],[1.0,1.0,1.0])     |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,491],[1.0,1.0,1.0])     |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])       |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])|\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |(500,[228,281,407],[1.0,1.0,1.0])     |\n",
      "+-------------------------------+-------------------------------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"countVec\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(1)\\\n",
    "  .setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[large, red, babushka, notebook]       |\n",
      "|[red, retrospot, oven, glove]          |\n",
      "|[red, retrospot, plate]                |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "tfIdfIn = tokenized\\\n",
    "  .where(\"array_contains(DescOut, 'red')\")\\\n",
    "  .select(\"DescOut\")\\\n",
    "  .limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"TFOut\")\\\n",
    "  .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "  .setInputCol(\"TFOut\")\\\n",
    "  .setOutputCol(\"IDFOut\")\\\n",
    "  .setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                |IDFOut                                                                                                           |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[50,52,415,6756,8005],[1.0,1.0,1.0,1.0,1.0])  |(10000,[50,52,415,6756,8005],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[52,4995,8737,9001],[1.0,1.0,1.0,1.0])        |(10000,[52,4995,8737,9001],[0.0,0.0,0.0,0.0])                                                                    |\n",
      "|[pin, cushion, babushka, red]          |(10000,[52,610,2490,7153],[1.0,1.0,1.0,1.0])         |(10000,[52,610,2490,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[52,547,6703,8448],[1.0,1.0,1.0,1.0])         |(10000,[52,547,6703,8448],[0.0,0.0,0.0,1.0116009116784799])                                                      |\n",
      "|[red, kitchen, scales]                 |(10000,[52,756,6452],[1.0,1.0,1.0])                  |(10000,[52,756,6452],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[52,2787,7022,7153],[1.0,1.0,1.0,1.0])        |(10000,[52,2787,7022,7153],[0.0,0.0,0.0,1.2992829841302609])                                                     |\n",
      "|[red, retrospot, oven, glove]          |(10000,[52,8242,8448,8667],[1.0,1.0,1.0,1.0])        |(10000,[52,8242,8448,8667],[0.0,0.0,1.0116009116784799,0.0])                                                     |\n",
      "|[red, retrospot, plate]                |(10000,[52,4925,8448],[1.0,1.0,1.0])                 |(10000,[52,4925,8448],[0.0,0.0,1.0116009116784799])                                                              |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DescOut</th>\n",
       "      <th>TFOut</th>\n",
       "      <th>IDFOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[gingham, heart, , doorstop, red]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[red, floral, feltcraft, shoulder, bag]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[alarm, clock, bakelike, red]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[pin, cushion, babushka, red]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[red, retrospot, mini, cases]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[red, kitchen, scales]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[gingham, heart, , doorstop, red]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[large, red, babushka, notebook]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[red, retrospot, oven, glove]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[red, retrospot, plate]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   DescOut  \\\n",
       "0        [gingham, heart, , doorstop, red]   \n",
       "1  [red, floral, feltcraft, shoulder, bag]   \n",
       "2            [alarm, clock, bakelike, red]   \n",
       "3            [pin, cushion, babushka, red]   \n",
       "4            [red, retrospot, mini, cases]   \n",
       "5                   [red, kitchen, scales]   \n",
       "6        [gingham, heart, , doorstop, red]   \n",
       "7         [large, red, babushka, notebook]   \n",
       "8            [red, retrospot, oven, glove]   \n",
       "9                  [red, retrospot, plate]   \n",
       "\n",
       "                                               TFOut  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              IDFOut  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.10983876287937165,-0.03447718722745776,0.000940057821571827]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.0072656965681484765,-0.018058971102748598,0.003386378288269043]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.047607143968343736,-0.0320490337908268,0.07224417026154697]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "  outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_6f4ed9b6b39d__output                  |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "|id |features      |PolynomialExpansion_70c2c7232672__output                                           |\n",
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|1  |[3.0,10.1,3.0]|[3.0,9.0,10.1,30.299999999999997,102.00999999999999,3.0,9.0,30.299999999999997,9.0]|\n",
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------------------------+\n",
      "|countVec                              |ChiSqSelector_9631cfd8c94f__output|\n",
      "+--------------------------------------+----------------------------------+\n",
      "|(500,[150,185,212],[1.0,1.0,1.0])     |(2,[],[])                         |\n",
      "|(500,[462,463,491],[1.0,1.0,1.0])     |(2,[],[])                         |\n",
      "|(500,[35,41,166],[1.0,1.0,1.0])       |(2,[],[])                         |\n",
      "|(500,[10,16,36,352],[1.0,1.0,1.0,1.0])|(2,[],[])                         |\n",
      "|(500,[228,281,407],[1.0,1.0,1.0])     |(2,[],[])                         |\n",
      "+--------------------------------------+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    "  .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "  .where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    "  .where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    "  .setFeaturesCol(\"countVec\")\\\n",
    "  .setLabelCol(\"CustomerId\")\\\n",
    "  .setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    "  .drop(\"customerId\", \"Description\", \"DescOut\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCAModel_5f0cec389764__output             |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show(5, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
