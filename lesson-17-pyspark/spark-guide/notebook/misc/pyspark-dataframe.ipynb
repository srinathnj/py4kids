{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe basics for PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import pandas as pd\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a sample dataframe from Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Owen</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Florence</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Lily</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>William</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId      Name     Sex  Survived\n",
       "0            1      Owen    male         0\n",
       "1            2  Florence  female         1\n",
       "2            3     Laina  female         1\n",
       "3            4      Lily  female         1\n",
       "4            5   William    male         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>71.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>53.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Age  Fare  Pclass\n",
       "0            1   22   7.3       3\n",
       "1            2   38  71.3       1\n",
       "2            3   26   7.9       3\n",
       "3            4   35  53.1       1\n",
       "4            5   35   8.0       3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pandas dataframe to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataframe verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 5 basic verbs for Spark, reproducing the basic verbs in R's dplyr package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select\n",
    "Takes either a list of column names or an unpacked list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "Filter takes column expression or SQL expression ---\n",
    "- Filter with column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter with SQL expression. Note the double and single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(\"Sex='female'\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutate, or creating new columns\n",
    "Creating new columns in Spark uses `.withColumn()`. I have yet found a convenient way to create multiple columns at once without chaining multiple `.withColumn()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize and group by\n",
    "To summarize or aggregate a dataframe, first I need to convert the dataframe to a `GroupedData` object with `groupby()`, then call the aggregate functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f0290f808d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "gdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can take the average of columns by passing an _unpacked_ list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call multiple aggregation functions at once, pass a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rename the columns `count(1)`, `avg(Age)` etc, use `toDF()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    gdf2\n",
    "    .agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrange (sort)\n",
    "Use the `.sort()` method to sort the dataframes. I haven't seen a good use case for this when I use Spark though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and unions\n",
    "There are two ways to combine dataframes --- joins and unions. The idea here is the same as joining and unioning tables in SQL. \n",
    "\n",
    "#### Joins\n",
    "\n",
    "For example, I can join the two titanic dataframes by the column `PassengerId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, ['PassengerId']).sort('PassengerId').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also join by conditions, but it creates duplicate column names if the keys have the same name. I found this behavior frustrating. For now, the only way to avoid this is to pass a list of join keys as in the previous cell. If I want to make nonequi joins, then I need to rename the keys before I join.\n",
    "\n",
    "#### Nonequi joins\n",
    "\n",
    "Here is an example of nonequi join. They can be _very_ slow due to skewed data, but this is one operation that Spark can do while Hive can not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|          1|    Owen|  male|       0|          1| 22| 7.3|     3|\n",
      "|          1|    Owen|  male|       0|          2| 38|71.3|     1|\n",
      "|          1|    Owen|  male|       0|          3| 26| 7.9|     3|\n",
      "|          1|    Owen|  male|       0|          4| 35|53.1|     1|\n",
      "|          1|    Owen|  male|       0|          5| 35| 8.0|     3|\n",
      "|          2|Florence|female|       1|          2| 38|71.3|     1|\n",
      "|          2|Florence|female|       1|          3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|          4| 35|53.1|     1|\n",
      "|          2|Florence|female|       1|          5| 35| 8.0|     3|\n",
      "|          3|   Laina|female|       1|          3| 26| 7.9|     3|\n",
      "|          3|   Laina|female|       1|          4| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1|          5| 35| 8.0|     3|\n",
      "|          4|    Lily|female|       1|          4| 35|53.1|     1|\n",
      "|          4|    Lily|female|       1|          5| 35| 8.0|     3|\n",
      "|          5| William|  male|       0|          5| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.PassengerId).show() # Note the duplicate col names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions\n",
    "\n",
    "`Union()` returns a dataframe from the union of two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of my iterative algorithms create chained `union()` objects. There is a potential catch that the execution plan may grow too long, which cause performance problems or errors. \n",
    "\n",
    "One common symptom of performance issues caused by chained unions in a for loop is it took longer and longer to iterate through the loop. In this case, `repartition()` and `checkpoint()` may help solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe input and output (I/O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two classes `pyspark.sql.DataFrameReader` and `pyspark.sql.DataFrameWriter` that handles dataframe I/O. Depending on the configuration, the files may be saved locally, through a Hive metasore, or to a Hadoop file system (HDFS). \n",
    "\n",
    "Common methods on saving dataframes to files include `saveAsTable()` for Hive tables and `saveAsFile()` for local or Hadoop file system.\n",
    "\n",
    "I will refer to the documentation on how to read and write dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spark.sql API\n",
    "\n",
    "Many of the operations that I showed can be accessed by writing SQL (Hive) queries in `spark.sql()`. This is also a convenient way to read Hive tables into Spark dataframes. To make an existing Spark dataframe usable for `spark.sql()`, I need to register said dataframe as a temporary table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp tables\n",
    "As an example, I can register the two dataframes as temp tables then join them through `spark.sql()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select \n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper\n",
    "\n",
    "In this section, I will go through some idea and useful tools associated with said ideas that I found helpful in tuning performance or debugging dataframes. The first of which is the difference between two types of operations: _transformations_ and _actions_, and a method `explain()` that prints out the execution plan of a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain(), transformations, and actions\n",
    "\n",
    "When I create a dataframe in PySpark, unlike Python objects, dataframes are lazy evaluated. What it means is that most operations are _transformations_ that modify the _execution plan_ on how Spark should handle the data, but the plan is not executed until we call and _action_.\n",
    "\n",
    "For example, if I want to join `df1` and `df2` on the key `PassengerId` as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L]\n"
     ]
    }
   ],
   "source": [
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[PassengerId#9L,Age#10L,Fare#11,Pclass#12L]\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Age#10L, Fare#11, Pclass#12L]\n",
      "+- *SortMergeJoin [PassengerId#0L], [PassengerId#9L], Inner\n",
      "   :- *Sort [PassengerId#0L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PassengerId#0L, 200)\n",
      "   :     +- *Filter isnotnull(PassengerId#0L)\n",
      "   :        +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L]\n",
      "   +- *Sort [PassengerId#9L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#9L, 200)\n",
      "         +- *Filter isnotnull(PassengerId#9L)\n",
      "            +- Scan ExistingRDD[PassengerId#9L,Age#10L,Fare#11,Pclass#12L]\n"
     ]
    }
   ],
   "source": [
    "dfj1 = df1.join(df2, ['PassengerId'])\n",
    "dfj1.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `join()` is a transformation that laid out a plan for Spark to join the two dataframes, but it wasn't executed unless I call an action, such as `.count()`, that has to go through the actual data defined by `df1` and `df2` in order to return a Python object (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfj1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I join through the `spark.sql()` API instead of calling `.join()`? In this case, both happen to generate the same execution plan. But it's not always like this. A good execution plan equals good performance, and I `explain()` a lot when I need to tune performance of Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Age#10L, Fare#11, Pclass#12L]\n",
      "+- *SortMergeJoin [PassengerId#0L], [PassengerId#9L], Inner\n",
      "   :- *Sort [PassengerId#0L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PassengerId#0L, 200)\n",
      "   :     +- *Filter isnotnull(PassengerId#0L)\n",
      "   :        +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L]\n",
      "   +- *Sort [PassengerId#9L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#9L, 200)\n",
      "         +- *Filter isnotnull(PassengerId#9L)\n",
      "            +- Scan ExistingRDD[PassengerId#9L,Age#10L,Fare#11,Pclass#12L]\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "    select \n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n",
    "dfj.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data persistence: cache() and checkpoint()\n",
    "\n",
    "#### caching\n",
    "\n",
    "Proper caching is the key to high performance Spark. I, however, still don't have good intuition on when to cache and when not to cache. \n",
    "\n",
    "A rule of thumb is that \n",
    "\n",
    "* **Cache a dataframe when it is used multiple times in the script.**\n",
    "\n",
    "Keep in mind that it is only cached _after the first action_, such as `saveAsTable()`. If for whatever reason I want to make sure the data is cached before I save the dataframe, then I have to call an action like `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: bigint, Name: string, Sex: string, Survived: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works as `.cache()` is an inplace method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if a dataframe is cached, check the `storageLevel` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To un-cache a dataframe, use `unpersist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 caching levels that can be fine-tuned with `persist()`, but I have not seen a use case where fine tuning has been worthwhile. Refer to the `persist()` documentation for detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpointing\n",
    "\n",
    "I said that sometimes chaining too many `union()` cause performance problem or even out of memory errors. `checkpoint()` truncates the execution plan and saves the checkpointed dataframe to a temporary location on the disk.\n",
    "\n",
    "[Jacek Laskowski recommends caching before checkpointing](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-checkpointing.html) so Spark doesn't have to read in the dataframe from disk after it's checkpointed.\n",
    "\n",
    "To use `checkpoint()`, I need to specify the temporary file location to save the datafame to by accessing the `sparkContext` object from `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"/checkpointdir\") # save to c:/checkpointdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I can join `df1` to itself 3 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#287, Sex#288, Survived#289L, Name#299, Sex#300, Survived#301L]\n",
      "+- *SortMergeJoin [PassengerId#0L], [PassengerId#298L], Inner\n",
      "   :- *Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#287, Sex#288, Survived#289L]\n",
      "   :  +- *SortMergeJoin [PassengerId#0L], [PassengerId#286L], Inner\n",
      "   :     :- *Sort [PassengerId#0L ASC NULLS FIRST], false, 0\n",
      "   :     :  +- Exchange hashpartitioning(PassengerId#0L, 200)\n",
      "   :     :     +- *Filter isnotnull(PassengerId#0L)\n",
      "   :     :        +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L]\n",
      "   :     +- *Sort [PassengerId#286L ASC NULLS FIRST], false, 0\n",
      "   :        +- Exchange hashpartitioning(PassengerId#286L, 200)\n",
      "   :           +- *Filter isnotnull(PassengerId#286L)\n",
      "   :              +- Scan ExistingRDD[PassengerId#286L,Name#287,Sex#288,Survived#289L]\n",
      "   +- *Sort [PassengerId#298L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#298L, 200)\n",
      "         +- *Filter isnotnull(PassengerId#298L)\n",
      "            +- Scan ExistingRDD[PassengerId#298L,Name#299,Sex#300,Survived#301L]\n"
     ]
    }
   ],
   "source": [
    "df = df1.join(df1, ['PassengerId'])\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also `checkpoint()` after the first join to truncate the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#314, Sex#315, Survived#316L, Name#335, Sex#336, Survived#337L]\n",
      "+- *SortMergeJoin [PassengerId#0L], [PassengerId#334L], Inner\n",
      "   :- *Filter isnotnull(PassengerId#0L)\n",
      "   :  +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L,Name#314,Sex#315,Survived#316L]\n",
      "   +- *Sort [PassengerId#334L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#334L, 200)\n",
      "         +- *Filter isnotnull(PassengerId#334L)\n",
      "            +- Scan ExistingRDD[PassengerId#334L,Name#335,Sex#336,Survived#337L]\n"
     ]
    }
   ],
   "source": [
    "df = df1.join(df1, ['PassengerId']).checkpoint()\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions and repartition()\n",
    "\n",
    "Another common cause of performance problems for me was having too many partitions. To check the number of partitions, use `.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe, despite having only 5 rows, has 4 partitions. This is too many. I can repartition to only 1 partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_repartitioned = df1.repartition(1)\n",
    "df1_repartitioned.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
