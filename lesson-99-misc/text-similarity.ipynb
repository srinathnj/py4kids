{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc46ab15",
   "metadata": {},
   "source": [
    "Google Bard:\n",
    "\n",
    "Comparing two approaches, first using bag-of-words similarity and then calculating Levenshtein distance for highly similar cases, is a reasonable strategy for determining the similarity between two text sequences. This approach can help you balance efficiency and accuracy.\n",
    "\n",
    "Bag-of-words comparison is a computationally efficient method that can quickly identify text pairs with high semantic similarity. However, it can be less sensitive to differences in the order of words. On the other hand, Levenshtein distance is more sensitive to the order of characters and can capture finer-grained differences in the structure of words or phrases.\n",
    "\n",
    "By initially using bag-of-words comparison, you can quickly identify text pairs that are likely to be similar and then focus on calculating Levenshtein distance only for those pairs. This can significantly improve the efficiency of your similarity calculations, especially when dealing with large datasets.\n",
    "\n",
    "Here's a summary of the proposed approach:\n",
    "\n",
    "Perform bag-of-words similarity: Calculate the similarity between two text sequences using a bag-of-words approach. This will provide a quick initial assessment of their similarity.\n",
    "\n",
    "Filter for highly similar pairs: Select pairs of text sequences that exhibit a high degree of similarity based on the bag-of-words comparison. This will narrow down the number of pairs requiring further analysis.\n",
    "\n",
    "Calculate Levenshtein distance: For the selected pairs, calculate the Levenshtein distance to obtain a more precise measure of their similarity, taking into account the order of characters.\n",
    "\n",
    "This approach offers a balance between efficiency and accuracy, allowing you to identify similar text pairs quickly while also capturing finer-grained differences in word order and structure for highly similar cases.\n",
    "\n",
    "If you're specifically interested in comparing the overall meaning or sentiment of the two strings, you might explore techniques based on natural language processing (NLP), such as word embeddings or semantic similarity measures. These approaches can provide a more comprehensive assessment of the semantic relatedness between the text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb1ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a992ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuation = string.punctuation + 'â€¦'  # Add ellipsis (...) to remove\n",
    "    return regex.sub(r'[{}]'.format(punctuation), '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addc9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    words1 = remove_punctuation(text1).lower().split()\n",
    "    words2 = remove_punctuation(text2).lower().split()\n",
    "    intersection = set(words1) & set(words2)\n",
    "    union = set(words1) | set(words2)\n",
    "    print(f\"intersect: {intersection}\")\n",
    "    print(f\"union: {union}\")\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29302d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersect: {'world', 'hello'}\n",
      "union: {'world', 'hello'}\n",
      "Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, world!\"\n",
    "text2 = \"WORLD Hello \"\n",
    "\n",
    "similarity = jaccard_similarity(text1, text2)\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "798023d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(text1, text2, ignore_punctuation=True, ignore_case=True):\n",
    "    if ignore_punctuation:\n",
    "        text1 = remove_punctuation(text1)\n",
    "        text2 = remove_punctuation(text2)\n",
    "    if ignore_case:\n",
    "        text1 = text1.lower()\n",
    "        text2 = text2.lower()\n",
    "        \n",
    "    return Levenshtein.distance(text1, text2)\n",
    "\n",
    "text1 = \"Hello, world!\"\n",
    "text2 = \"hello word\"  # \"Hi there!\"\n",
    "\n",
    "# text1 = \"Hello, world!\"\n",
    "# text2 = \"WORLD Hello \"\n",
    "\n",
    "distance = levenshtein_distance(text1, text2)\n",
    "print(\"Levenshtein distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a51004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    words1 = text1.split()\n",
    "    words2 = text2.split()\n",
    "    intersection = set(words1) & set(words2)\n",
    "    union = set(words1) | set(words2)\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "def edit_distance(text1, text2, ignore_punctuation=True, ignore_case=True, ignore_order=True):\n",
    "    if ignore_punctuation:\n",
    "        text1 = remove_punctuation(text1)\n",
    "        text2 = remove_punctuation(text2)\n",
    "    if ignore_case:\n",
    "        text1 = text1.lower()\n",
    "        text2 = text2.lower()\n",
    "        \n",
    "    if ignore_order:\n",
    "        return jaccard_similarity(text1, text2)\n",
    "    else:\n",
    "        return Levenshtein.distance(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce6c8b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.0\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, world!\"\n",
    "text2 = \"halo word\"  # \"Hi there!\"\n",
    "sim = edit_distance(text1, text2)\n",
    "print(f\"similarity: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a431e107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 3\n"
     ]
    }
   ],
   "source": [
    "sim = edit_distance(text1, text2, ignore_order=False)\n",
    "print(f\"similarity: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d6abb",
   "metadata": {},
   "source": [
    "Semantic similarity between to sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904b4b4",
   "metadata": {},
   "source": [
    "see https://github.com/mmihaltz/word2vec-GoogleNews-vectors/tree/master\n",
    "\n",
    "too big 3GB\n",
    "https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d010789d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7200\\241929660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load pre-trained word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Calculate semantic similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m         \"\"\"\n\u001b[1;32m-> 1629\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Calculate semantic similarity\n",
    "text1 = \"Hello, world!\"\n",
    "text2 = \"Hi there!\"\n",
    "\n",
    "similarity = model.similarity(text1, text2)\n",
    "print(\"Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8589f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_s = \"\"\"\n",
    "QUAD\n",
    "LYTS\n",
    "NNBR\n",
    "EBF\n",
    "AMRX\n",
    "ETON\n",
    "FBIO\n",
    "HEES\n",
    "AGX\n",
    "AMPH\n",
    "ARCB\n",
    "AZZ\n",
    "BBIO\n",
    "CARR\n",
    "CRS\n",
    "EOLS\n",
    "ETN\n",
    "FLS\n",
    "FUL\n",
    "GWW\n",
    "INSW\n",
    "MTRN\n",
    "PKE\n",
    "PKOH\n",
    "UFPI\n",
    "USLM\n",
    "VMC\n",
    "VRSK\n",
    "WLDN\n",
    "CABA\n",
    "CMPR\n",
    "IESC\n",
    "NPK\n",
    "ROCK\n",
    "SFL\n",
    "SNA\n",
    "SPXC\n",
    "SXI\n",
    "VRRM\n",
    "WAB\n",
    "ALG\n",
    "AMWD\n",
    "BECN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "329c4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [i.strip() for i in ticker_s.split(\"\\n\") if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7476c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUAD LYTS NNBR EBF AMRX ETON FBIO HEES AGX AMPH ARCB AZZ BBIO CARR CRS EOLS ETN FLS FUL GWW INSW MTRN PKE PKOH UFPI USLM VMC VRSK WLDN CABA CMPR IESC NPK ROCK SFL SNA SPXC SXI VRRM WAB ALG AMWD BECN\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c00e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
