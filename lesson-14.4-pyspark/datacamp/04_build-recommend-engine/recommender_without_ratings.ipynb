{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSD summary statistics\n",
    "\n",
    "Let's get familiar with the Million Songs Echo Nest Taste Profile data subset. For purposes of this course, we'll just call it the Million Songs dataset or msd. Let's get the number of users and the number of songs. Let's also see which songs have the most plays from this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "msd.show()\n",
    "\n",
    "# Count the number of distinct userIds\n",
    "user_count = msd.select(\"userId\").distinct().count()\n",
    "print(\"Number of users: \", user_count)\n",
    "\n",
    "# Count the number of distinct songIds\n",
    "song_count = msd.select(\"songId\").distinct().count()\n",
    "print(\"Number of songs: \", song_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped summary statistics\n",
    "\n",
    "In this exercise, we are going to combine the .groupBy() and .filter() methods that you've used previously to calculate the min() and avg() number of users that have rated each song, and the min() and avg() number of songs that each user has rated.\n",
    "\n",
    "Because our data now includes 0's for items not yet consumed, we'll need to .filter() them out when doing grouped summary statistics like this. The msd dataset is provided for you here. The col(), min(), and avg() functions from pyspark.sql.functions have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Zeros\n",
    "\n",
    "Many recommendation engines use implicit ratings. In many cases these datasets don't include behavior counts for items that a user has never purchased. In these cases, you'll need to add them and include zeros. The dataframe Z is provided for you. It contains userId's, productId's and num_purchases which is the number of times a user has purchased a specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "Z.show()\n",
    "\n",
    "# Extract distinct userIds and productIds\n",
    "users = Z.select(\"userId\").distinct()\n",
    "products = Z.select(\"productId\").distinct()\n",
    "\n",
    "# Cross join users and products\n",
    "cj = users.crossJoin(products)\n",
    "\n",
    "# Join cj and Z\n",
    "Z_expanded = cj.join(Z, [\"userId\", \"productId\"], \"left\").fillna(0)\n",
    "\n",
    "# View Z_expanded\n",
    "Z_expanded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Implicit Ratings Models\n",
    "\n",
    "### Specify ALS Hyperparameters\n",
    "\n",
    "You're now going to build your first implicit rating recommendation engine using ALS. To do this, you will first tell Spark what values you want it to try when finding the best model.\n",
    "\n",
    "Four empty lists are provided below. You will fill them with specific values that Spark can use to build several different ALS models. In the next exercise, you'll tell Spark to build out these models using the lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the lists below\n",
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Implicit Models\n",
    "\n",
    "Now that you have all of your hyperparameter values specified, let's have Spark build enough models to test each combination. To facilitate this, a for loop is provided here. Follow the instructions below to automatically create these ALS models. In subsequent exercises you will run these models on test datasets to see which one performs the best.\n",
    "\n",
    "The ALS algorithm is already imported for you. The lists you created in the last exercise (ranks, maxIters, regParams, alphas) have been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"userId\", itemCol= \"songId\", ratingCol= \"num_plays\", rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Cross-Validated Implicit ALS Model\n",
    "\n",
    "Now that we have several ALS models, each with a different set of hyperparameter values, we can train them on a training portion of the msd dataset using cross validation, and then run them on a test set of data and evaluate how well each one performs using the ROEM function discussed earlier. Unfortunately, this takes too much time for this exercise, so it has been done separately. But for your reference you can evaluate your model_list using the following loop (we are using the msd dataset in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(training, test) = msd.randomSplit([0.8, 0.2])\n",
    "\n",
    "#Building 5 folds within the training set.\n",
    "train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "# Loops through all models and all folds\n",
    "for model in model_list:\n",
    "    for ft_pair in foldlist:\n",
    "\n",
    "        # Fits model to fold within training data\n",
    "        fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "        # Generates predictions using fitted_model on respective CV test data\n",
    "        predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "        # Generates and prints a ROEM metric CV test data\n",
    "        r = ROEM(predictions)\n",
    "        print (\"ROEM: \", r)\n",
    "\n",
    "    # Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(training)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "    # Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM: \", v_ROEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy\n",
    "\n",
    "# Find the index of the smallest ROEM\n",
    "i = numpy.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", i)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Parameters\n",
    "\n",
    "You've now tested 192 different models on the msd dataset, and you found the best ROEM and its respective model (model 38).\n",
    "\n",
    "You now need to extract the hyperparameters. The model_list you created previously is provided here. It contains all 192 models you generated. Use the instructions below to extract the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best_model\n",
    "best_model = model_list[38]\n",
    "\n",
    "# Extract the Rank\n",
    "print (\"Rank: \", best_model.getRank())\n",
    "\n",
    "# Extract the MaxIter value\n",
    "print (\"MaxIter: \", best_model.getMaxIter())\n",
    "\n",
    "# Extract the RegParam value\n",
    "print (\"RegParam: \", best_model.getRegParam())\n",
    "\n",
    "# Extract the Alpha value\n",
    "print (\"Alpha: \", best_model.getAlpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of binary implicit ratings\n",
    "\n",
    "### Binary Model Performance\n",
    "\n",
    "You've already built several ALS models, so we won't do that again. An implicit ALS model has already been fitted to the binary ratings of the MovieLens dataset. Let's look at the binary_test_predictions from this model to see what we can learn.\n",
    "\n",
    "The ROEM() function has been defined for you. Feel free to run help(ROEM) in the console if you want more details on how to execute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the col function\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Look at the test predictions\n",
    "binary_test_predictions.show()\n",
    "\n",
    "# Evaluate ROEM on test predictions\n",
    "ROEM(binary_test_predictions)\n",
    "\n",
    "# Look at user 42's test predictions\n",
    "binary_test_predictions.filter(col(\"userId\") == 42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations From Binary Data\n",
    "\n",
    "So you see from the ROEM, these models can still generate meaningful test predictions. Let's look at the actual recommendations now.\n",
    "\n",
    "The col function from the pyspark.sql.functions class has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View user 26's original ratings\n",
    "print (\"User 26 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 26's recommendations\n",
    "print (\"User 26 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 99's original ratings\n",
    "print (\"User 99 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 99).show()\n",
    "\n",
    "# View user 99's recommendations\n",
    "print (\"User 99 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 99).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
