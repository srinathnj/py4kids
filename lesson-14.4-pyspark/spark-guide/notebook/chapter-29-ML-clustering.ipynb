{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"chapter-29-ML-clustering\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "import os\n",
    "SPARK_BOOK_DATA_PATH = os.environ['SPARK_BOOK_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f05ac1f1ac8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, features: vector]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler()\\\n",
    "  .setInputCols([\"Quantity\", \"UnitPrice\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "sales = va.transform(spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(SPARK_BOOK_DATA_PATH + \"/data/retail-data/by-day/*.csv\")\n",
    "  .limit(50)\n",
    "  .coalesce(1)\n",
    "  .where(\"Description IS NOT NULL\"))\n",
    "\n",
    "sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should match to limit(50) from spark.read.limit(50)\n",
    "sales.count()  # to trigger cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |features   |\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|580538   |23084    |RABBIT NIGHT LIGHT             |48      |2011-12-05 08:38:00|1.79     |14075.0   |United Kingdom|[48.0,1.79]|\n",
      "|580538   |23077    |DOUGHNUT LIP GLOSS             |20      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|[20.0,1.25]|\n",
      "|580538   |22906    |12 MESSAGE CARDS WITH ENVELOPES|24      |2011-12-05 08:38:00|1.65     |14075.0   |United Kingdom|[24.0,1.65]|\n",
      "|580538   |21914    |BLUE HARMONICA IN BOX          |24      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|[24.0,1.25]|\n",
      "|580538   |22467    |GUMBALL COAT RACK              |6       |2011-12-05 08:38:00|2.55     |14075.0   |United Kingdom|[6.0,2.55] |\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 7969353092125344463)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "km = KMeans().setK(5)\n",
    "print (km.explainParams())\n",
    "kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "summary = kmModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 29, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print (summary.clusterSizes) # number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 8, 13, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkm = BisectingKMeans().setK(5).setMaxIter(5)\n",
    "bkmModel = bkm.fit(sales)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "summary = bkmModel.summary\n",
    "print (summary.clusterSizes) # number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "k: Number of independent Gaussians in the mixture model. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "seed: random seed. (default: -7090211980209472397)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.01)\n",
      "[0.16503937777770641, 0.35496420094056985, 0.06003637101912308, 0.1999636297743671, 0.21999642048823354]\n",
      "+--------------------+--------------------+\n",
      "|                mean|                 cov|\n",
      "+--------------------+--------------------+\n",
      "|[2.54180583818530...|0.785769315153778...|\n",
      "|[5.07243095740621...|2.059950971034034...|\n",
      "|[43.9877864408847...|32.22707068867282...|\n",
      "|[23.1998836372414...|2.560279258630084...|\n",
      "|[11.6364190345020...|1.322132750446848...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gmm = GaussianMixture().setK(5)\n",
    "print (gmm.explainParams())\n",
    "model = gmm.fit(sales)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "summary = model.summary\n",
    "print (model.weights)\n",
    "model.gaussiansDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 18, 3, 10, 11]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.clusterSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[1.37632400885157...|\n",
      "|[4.89041912245635...|\n",
      "|[1.67299627008735...|\n",
      "|[7.43321003719004...|\n",
      "|[1.46369160111044...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.probability.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.drop(\"features\"))\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"features\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(0)\\\n",
    "  .setMinDF(0)\\\n",
    "  .setBinary(True)\n",
    "cvFitted = cv.fit(tokenized)\n",
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "docConcentration: Concentration parameter (commonly named \"alpha\") for the prior placed on documents' distributions over topics (\"theta\"). (undefined)\n",
      "featuresCol: features column name. (default: features)\n",
      "k: The number of topics (clusters) to infer. Must be > 1. (default: 10, current: 10)\n",
      "keepLastCheckpoint: (For EM optimizer) If using checkpointing, this indicates whether to keep the last checkpoint. If false, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. (default: True)\n",
      "learningDecay: Learning rate, set as anexponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. (default: 0.51)\n",
      "learningOffset: A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less (default: 1024.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 5)\n",
      "optimizeDocConcentration: Indicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. (default: True)\n",
      "optimizer: Optimizer or inference algorithm used to estimate the LDA model.  Supported: online, em (default: online)\n",
      "seed: random seed. (default: 7673890338921026109)\n",
      "subsamplingRate: Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. (default: 0.05)\n",
      "topicConcentration: Concentration parameter (commonly named \"beta\" or \"eta\") for the prior placed on topic' distributions over terms. (undefined)\n",
      "topicDistributionCol: Output column with estimates of the topic mixture distribution for each document (often called \"theta\" in the literature). Returns a vector of zeros for an empty document. (default: topicDistribution)\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA().setK(10).setMaxIter(5)\n",
    "print (lda.explainParams())\n",
    "model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+\n",
      "|topic|  termIndices|         termWeights|\n",
      "+-----+-------------+--------------------+\n",
      "|    0|   [4, 6, 17]|[0.01166617595444...|\n",
      "|    1|  [13, 9, 66]|[0.01091339619437...|\n",
      "|    2|[15, 131, 45]|[0.00897001978399...|\n",
      "|    3| [6, 125, 78]|[0.00902243209856...|\n",
      "|    4|[103, 55, 62]|[0.00933169978592...|\n",
      "|    5|  [11, 5, 23]|[0.01496249652116...|\n",
      "|    6|   [7, 16, 2]|[0.01140631424514...|\n",
      "|    7| [28, 73, 69]|[0.01535561351823...|\n",
      "|    8|    [0, 3, 1]|[0.01802752067282...|\n",
      "|    9| [46, 90, 30]|[0.01169076631125...|\n",
      "+-----+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "model.describeTopics(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'hot',\n",
       " 'vintage',\n",
       " 'bottle',\n",
       " 'paperweight',\n",
       " '6',\n",
       " 'home',\n",
       " 'doormat',\n",
       " 'landmark',\n",
       " 'bicycle',\n",
       " 'frame',\n",
       " 'ribbons',\n",
       " '',\n",
       " 'classic',\n",
       " 'rose',\n",
       " 'kit',\n",
       " 'leaf',\n",
       " 'sweet',\n",
       " 'bag',\n",
       " 'airline',\n",
       " 'doorstop',\n",
       " 'light',\n",
       " 'in',\n",
       " 'christmas',\n",
       " 'heart',\n",
       " 'calm',\n",
       " 'set',\n",
       " 'keep',\n",
       " 'balloons',\n",
       " 'night',\n",
       " 'lights',\n",
       " '12',\n",
       " 'tin',\n",
       " 'english',\n",
       " 'caravan',\n",
       " 'stuff',\n",
       " 'tidy',\n",
       " 'oxford',\n",
       " 'full',\n",
       " 'cottage',\n",
       " 'notting',\n",
       " 'drawer',\n",
       " 'mushrooms',\n",
       " 'chrome',\n",
       " 'champion',\n",
       " 'amelie',\n",
       " 'mini',\n",
       " 'the',\n",
       " 'giant',\n",
       " 'design',\n",
       " 'elegant',\n",
       " 'tins',\n",
       " 'jet',\n",
       " 'fairy',\n",
       " \"50's\",\n",
       " 'holder',\n",
       " 'message',\n",
       " 'blue',\n",
       " 'storage',\n",
       " 'tier',\n",
       " 'covent',\n",
       " 'world',\n",
       " 'skulls',\n",
       " 'font',\n",
       " 'hearts',\n",
       " 'skull',\n",
       " 'clips',\n",
       " 'bell',\n",
       " 'red',\n",
       " 'party',\n",
       " 'chalkboard',\n",
       " 'save',\n",
       " '4',\n",
       " 'coloured',\n",
       " 'poppies',\n",
       " 'garden',\n",
       " 'nine',\n",
       " 'girl',\n",
       " 'shimmering',\n",
       " 'doughnut',\n",
       " 'dog',\n",
       " '3',\n",
       " 'tattoos',\n",
       " 'chilli',\n",
       " 'coat',\n",
       " 'torch',\n",
       " 'sunflower',\n",
       " 'tale',\n",
       " 'cards',\n",
       " 'puncture',\n",
       " 'woodland',\n",
       " 'bomb',\n",
       " 'knack',\n",
       " 'lip',\n",
       " 'collage',\n",
       " 'rabbit',\n",
       " 'sex',\n",
       " 'of',\n",
       " 'rack',\n",
       " 'wall',\n",
       " 'cracker',\n",
       " 'scottie',\n",
       " 'hill',\n",
       " 'led',\n",
       " 'black',\n",
       " 'art',\n",
       " 'envelopes',\n",
       " 'flytrap',\n",
       " 'box',\n",
       " 'pinks',\n",
       " 'camouflage',\n",
       " 'gingham',\n",
       " 'popcorn',\n",
       " 'with',\n",
       " 'knick',\n",
       " 'empire',\n",
       " 'grow',\n",
       " 'fancy',\n",
       " 'plate',\n",
       " 'natural',\n",
       " 'feltcraft',\n",
       " 'brown',\n",
       " 'paisley',\n",
       " 'repair',\n",
       " 'gumball',\n",
       " 'white',\n",
       " 'regency',\n",
       " 'cakestand',\n",
       " 'rocket',\n",
       " 'harmonica',\n",
       " 'a',\n",
       " 'or',\n",
       " 'transfer',\n",
       " 'street',\n",
       " 'planet',\n",
       " 'office',\n",
       " 'gloss',\n",
       " 'slate',\n",
       " 'towel',\n",
       " 'tea',\n",
       " 'breakfast']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvFitted.vocabulary\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
